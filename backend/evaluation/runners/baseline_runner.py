"""
Baseline í‰ê°€ ì‹¤í–‰ê¸° v2.1
- ì‹¤í–‰: python -m evaluation.runners.baseline_runner
- íŒŒì¼ ì €ì¥: evaluation/results/baseline/

v2.1 ë³€ê²½ì‚¬í•­:
- ë…¼ë¬¸ ì¸ìš© ì¸¡ì • ë¶„ë¦¬:
  - citation_strict: ì €ìëª… ì§ì ‘ ì¸ìš© (Fine-tuning íš¨ê³¼ ì¸¡ì •)
  - concept_application: ì „ë¬¸ ê°œë… ì ìš© (í”„ë¡¬í”„íŠ¸ í’ˆì§ˆ ì¸¡ì •)
- ìš´ë™ ë¶„ì„ í•¨ìˆ˜ í˜¸ì¶œ ì¸ì ìˆ˜ì •

v2 ë³€ê²½ì‚¬í•­:
- 6ë“±ê¸‰ ì»¨ë””ì…˜ ê¸°ì¤€ ì ìš© (ì‹¤ì œ ì„œë¹„ìŠ¤ health_interpreter.py ê¸°ì¤€)
- ìƒˆ í‰ê°€ ì§€í‘œ ì¶”ê°€:
  - ì‘ë‹µ êµ¬ì¡° ì¼ì¹˜ìœ¨ (has_condition_score, has_grade, has_judgment_basis)
  - ì „ë¬¸ ê¸°ì¤€ ì¸ìš©ìœ¨ (should_cite_buchheit, should_cite_milewski)
  - ì»¨ë””ì…˜ ë“±ê¸‰ ì •í™•ë„
  - ì‘ë‹µ ê¸¸ì´ ì ì ˆì„±
- v2 í…ŒìŠ¤íŠ¸ ë°ì´í„° í˜•ì‹ í˜¸í™˜ (06_generate_test_data_v2.py)
"""

import json
import os
import requests
from pathlib import Path
from datetime import datetime, timedelta
import sys
import re

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from evaluation.config import API_BASE_URL, TEST_USER_ID, EVALUATION_ROUNDS, RESULTS_DIR
from evaluation.metrics.response_quality import ResponseQualityMetrics
from evaluation.metrics.performance import PerformanceMetrics
from evaluation.metrics.rag_quality import RAGQualityMetrics


# ============================================
# 6ë“±ê¸‰ ì»¨ë””ì…˜ ê¸°ì¤€ (ì‹¤ì œ ì„œë¹„ìŠ¤ ê¸°ì¤€)
# ============================================
CONDITION_GRADES_V2 = {
    "optimal": {"min": 80, "grade": "A", "label": "ë§¤ìš° ìš°ìˆ˜"},
    "good": {"min": 70, "grade": "B", "label": "ìš°ìˆ˜"},
    "moderate_plus": {"min": 55, "grade": "C+", "label": "ë³´í†µ ì´ìƒ"},
    "moderate": {"min": 45, "grade": "C", "label": "ë³´í†µ"},
    "caution": {"min": 35, "grade": "D", "label": "ê°œì„  í•„ìš”"},
    "warning": {"min": 0, "grade": "F", "label": "ì£¼ì˜ í•„ìš”"},
}


# ============================================
# ì „ë¬¸ ê¸°ì¤€ ì¸ìš© í‚¤ì›Œë“œ (v2.1 - ë¶„ë¦¬)
# ============================================

# ì—„ê²©í•œ ì¸ìš© ê²€ì‚¬ (ì €ìëª…ë§Œ) - Fine-tuning íš¨ê³¼ ì¸¡ì •ìš©
PROFESSIONAL_REFERENCES_STRICT = {
    "buchheit": ["Buchheit", "buchheit", "ë¶€íí•˜ì´íŠ¸"],
    "milewski": ["Milewski", "milewski", "ë°€ë ˆë¸ŒìŠ¤í‚¤"],
    "karvonen": ["Karvonen", "karvonen", "ì¹´ë³´ë„¨"],
    "acsm": ["ACSM", "acsm"],
}

# ê°œë… ì ìš© ê²€ì‚¬ (ì „ë¬¸ ê°œë… í‚¤ì›Œë“œ) - í”„ë¡¬í”„íŠ¸ í’ˆì§ˆ ì¸¡ì •ìš©
CONCEPT_KEYWORDS = {
    "buchheit_concept": [
        "+10bpm",
        "10bpm ì´ìƒ",
        "í”¼ë¡œ ì‹ í˜¸",
        "ì•ˆì •ì‹œ ì‹¬ë°•",
        "ê³¼í›ˆë ¨",
        "HRV ì €í•˜",
    ],
    "milewski_concept": [
        "1.7ë°°",
        "ë¶€ìƒ ìœ„í—˜",
        "8ì‹œê°„ ë¯¸ë§Œ",
        "ìˆ˜ë©´ ë¶€ì¡±",
        "ë©´ì—­ë ¥ ì €í•˜",
    ],
    "karvonen_concept": [
        "ëª©í‘œ ì‹¬ë°•ìˆ˜",
        "ì—¬ìœ ì‹¬ë°•ìˆ˜",
        "HRR",
        "ìš´ë™ ê°•ë„ ê³µì‹",
        "ìµœëŒ€ì‹¬ë°•ìˆ˜",
    ],
    "acsm_concept": [
        "ê¶Œì¥ëŸ‰",
        "ê°€ì´ë“œë¼ì¸",
        "ì£¼ë‹¹ 150ë¶„",
        "ì¤‘ê°•ë„ ìœ ì‚°ì†Œ",
    ],
}


# ============================================
# í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ê±´ê°• ë°ì´í„° (7ì¼ì¹˜)
# ============================================
SAMPLE_HEALTH_DATA = [
    {
        "date_offset": 0,
        "raw": {
            "heart_rate": 72,
            "resting_heart_rate": 62,
            "sleep_hr": 7.5,
            "steps": 8500,
            "distance_km": 6.2,
            "active_calories": 380,
            "oxygen_saturation": 97,
            "weight": 72.0,
            "bmi": 23.5,
        },
    },
    {
        "date_offset": -1,
        "raw": {
            "heart_rate": 68,
            "resting_heart_rate": 60,
            "sleep_hr": 8.0,
            "steps": 10200,
            "distance_km": 7.8,
            "active_calories": 450,
            "oxygen_saturation": 98,
            "weight": 71.8,
            "bmi": 23.4,
        },
    },
    {
        "date_offset": -2,
        "raw": {
            "heart_rate": 75,
            "resting_heart_rate": 65,
            "sleep_hr": 6.5,
            "steps": 6000,
            "distance_km": 4.5,
            "active_calories": 280,
            "oxygen_saturation": 96,
            "weight": 72.2,
            "bmi": 23.6,
        },
    },
]


class BaselineRunner:
    def __init__(self):
        import os

        os.environ["EVAL_MODE"] = "baseline"
        print(f"[INFO] EVAL_MODE = baseline")

        self.base_url = API_BASE_URL
        self.user_id = TEST_USER_ID
        self.results = {"health": [], "exercise": [], "chat": []}
        self.summary = {}
        self.test_data_ids = []

        from app.services.chat_service import ChatService

        self.chat_service = ChatService()

        self._load_service_modules()

    def _load_service_modules(self):
        """ì„œë¹„ìŠ¤ ëª¨ë“ˆ ë™ì  ë¡œë“œ"""
        try:
            from app.core.health_interpreter import interpret_health_data

            self.interpret_health_data = interpret_health_data
            print("âœ… health_interpreter ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
        except ImportError as e:
            print(f"âš ï¸ health_interpreter ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.interpret_health_data = None

        try:
            from app.core.llm_analysis import run_llm_analysis

            self.run_llm_analysis = run_llm_analysis
            print("âœ… llm_analysis ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
        except ImportError as e:
            print(f"âš ï¸ llm_analysis ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.run_llm_analysis = None

        try:
            from app.core.vector_store import save_daily_summary, collection

            self.save_daily_summary = save_daily_summary
            self.chroma_collection = collection
            print("âœ… vector_store ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
        except ImportError as e:
            print(f"âš ï¸ vector_store ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.save_daily_summary = None
            self.chroma_collection = None

    # ============================================
    # ìƒˆ í‰ê°€ ì§€í‘œ í•¨ìˆ˜ë“¤
    # ============================================

    def _score_to_grade_v2(self, score: int) -> str:
        """ì ìˆ˜ â†’ ë“±ê¸‰ ë³€í™˜ (6ë“±ê¸‰)"""
        if score >= 80:
            return "optimal"
        elif score >= 70:
            return "good"
        elif score >= 55:
            return "moderate_plus"
        elif score >= 45:
            return "moderate"
        elif score >= 35:
            return "caution"
        else:
            return "warning"

    def _check_structure_match(self, response: str, expected: dict) -> dict:
        """ì‘ë‹µ êµ¬ì¡° ì¼ì¹˜ ì—¬ë¶€ í™•ì¸"""
        checks = {
            "has_condition_score": False,
            "has_grade": False,
            "has_judgment_basis": False,
        }

        if not response:
            return checks

        response_lower = response.lower()

        # ì»¨ë””ì…˜ ì ìˆ˜ í¬í•¨ ì—¬ë¶€ (ì˜ˆ: "75/100", "ì ìˆ˜: 75")
        if re.search(r"\d+/100|\d+ì |ì ìˆ˜[:\s]*\d+", response):
            checks["has_condition_score"] = True

        # ë“±ê¸‰ í¬í•¨ ì—¬ë¶€ (ì˜ˆ: "Aë“±ê¸‰", "ë§¤ìš° ìš°ìˆ˜", "ì–‘í˜¸")
        grade_keywords = [
            "ë“±ê¸‰",
            "ë§¤ìš° ìš°ìˆ˜",
            "ìš°ìˆ˜",
            "ë³´í†µ",
            "ê°œì„  í•„ìš”",
            "ì£¼ì˜ í•„ìš”",
            "ì–‘í˜¸",
            "ìµœì ",
            "ê²½ê³ ",
        ]
        if any(kw in response for kw in grade_keywords):
            checks["has_grade"] = True

        # íŒë‹¨ ê·¼ê±° í¬í•¨ ì—¬ë¶€ (ì˜ˆ: "íŒë‹¨ ê·¼ê±°", "ì´ìœ ", "ë¶„ì„")
        basis_keywords = [
            "íŒë‹¨ ê·¼ê±°",
            "ê·¼ê±°",
            "ì´ìœ ",
            "ë•Œë¬¸",
            "ë¶„ì„",
            "â†’",
            "âœ…",
            "âš ï¸",
            "ğŸš¨",
        ]
        if any(kw in response for kw in basis_keywords):
            checks["has_judgment_basis"] = True

        return checks

    def _check_citation_strict(self, response: str, expected: dict) -> dict:
        """ì—„ê²©í•œ ë…¼ë¬¸ ì¸ìš© í™•ì¸ (ì €ìëª…ë§Œ) - Fine-tuning íš¨ê³¼ ì¸¡ì •"""
        result = {
            "buchheit_cited": False,
            "milewski_cited": False,
            "karvonen_cited": False,
            "acsm_cited": False,
            "should_cite_buchheit": expected.get("should_cite_buchheit", False),
            "should_cite_milewski": expected.get("should_cite_milewski", False),
            "citation_strict_score": 0.0,
        }

        if not response:
            return result

        # ê° ì €ìëª… ì¸ìš© í™•ì¸
        for keyword in PROFESSIONAL_REFERENCES_STRICT["buchheit"]:
            if keyword in response:
                result["buchheit_cited"] = True
                break

        for keyword in PROFESSIONAL_REFERENCES_STRICT["milewski"]:
            if keyword in response:
                result["milewski_cited"] = True
                break

        for keyword in PROFESSIONAL_REFERENCES_STRICT["karvonen"]:
            if keyword in response:
                result["karvonen_cited"] = True
                break

        for keyword in PROFESSIONAL_REFERENCES_STRICT["acsm"]:
            if keyword in response:
                result["acsm_cited"] = True
                break

        # ì—„ê²©í•œ ì¸ìš© ì ìˆ˜ ê³„ì‚° (ê¸°ëŒ€í•˜ëŠ” ì¸ìš©ë§Œ ì²´í¬)
        expected_citations = 0
        matched_citations = 0

        if result["should_cite_buchheit"]:
            expected_citations += 1
            if result["buchheit_cited"]:
                matched_citations += 1

        if result["should_cite_milewski"]:
            expected_citations += 1
            if result["milewski_cited"]:
                matched_citations += 1

        if expected_citations > 0:
            result["citation_strict_score"] = matched_citations / expected_citations
        else:
            result["citation_strict_score"] = 1.0  # ì¸ìš© í•„ìš” ì—†ìœ¼ë©´ ë§Œì 

        return result

    def _check_concept_application(self, response: str, expected: dict) -> dict:
        """ì „ë¬¸ ê°œë… ì ìš© í™•ì¸ - í”„ë¡¬í”„íŠ¸ í’ˆì§ˆ ì¸¡ì •"""
        result = {
            "buchheit_concept_applied": False,
            "milewski_concept_applied": False,
            "karvonen_concept_applied": False,
            "acsm_concept_applied": False,
            "concept_score": 0.0,
            "concepts_found": [],
        }

        if not response:
            return result

        concepts_applied = 0
        total_concept_types = 0

        # Buchheit ê°œë… í™•ì¸
        if expected.get("should_cite_buchheit", False):
            total_concept_types += 1
            for keyword in CONCEPT_KEYWORDS["buchheit_concept"]:
                if keyword in response:
                    result["buchheit_concept_applied"] = True
                    result["concepts_found"].append(keyword)
                    concepts_applied += 1
                    break

        # Milewski ê°œë… í™•ì¸
        if expected.get("should_cite_milewski", False):
            total_concept_types += 1
            for keyword in CONCEPT_KEYWORDS["milewski_concept"]:
                if keyword in response:
                    result["milewski_concept_applied"] = True
                    result["concepts_found"].append(keyword)
                    concepts_applied += 1
                    break

        # Karvonen ê°œë… í™•ì¸ (ìš´ë™ ë¶„ì„ìš©)
        if expected.get("has_karvonen", False):
            total_concept_types += 1
            for keyword in CONCEPT_KEYWORDS["karvonen_concept"]:
                if keyword in response:
                    result["karvonen_concept_applied"] = True
                    result["concepts_found"].append(keyword)
                    concepts_applied += 1
                    break

        # ê°œë… ì ìš© ì ìˆ˜ ê³„ì‚°
        if total_concept_types > 0:
            result["concept_score"] = concepts_applied / total_concept_types
        else:
            result["concept_score"] = 1.0  # ê°œë… ì ìš© í•„ìš” ì—†ìœ¼ë©´ ë§Œì 

        return result

    def _check_length_appropriate(self, response: str, expected: dict) -> dict:
        """ì‘ë‹µ ê¸¸ì´ ì ì ˆì„± í™•ì¸"""
        min_len = expected.get("min_length", 50)
        max_len = expected.get("max_length", 500)
        actual_len = len(response) if response else 0

        return {
            "actual_length": actual_len,
            "min_length": min_len,
            "max_length": max_len,
            "is_appropriate": min_len <= actual_len <= max_len,
            "length_score": (
                1.0
                if min_len <= actual_len <= max_len
                else max(0, 1 - abs(actual_len - (min_len + max_len) / 2) / max_len)
            ),
        }

    def _check_condition_grade_accuracy(self, response: dict, expected: dict) -> dict:
        """ì»¨ë””ì…˜ ë“±ê¸‰ ì •í™•ë„ í™•ì¸ (6ë“±ê¸‰)"""
        result = {
            "expected_level": expected.get("condition_level", ""),
            "actual_level": "",
            "is_match": False,
            "is_adjacent": False,  # ì¸ì ‘ ë“±ê¸‰ ì—¬ë¶€
        }

        if not isinstance(response, dict) or "error" in response:
            return result

        # ì‘ë‹µì—ì„œ ì ìˆ˜ ì¶”ì¶œ
        health_score = response.get("health_score", {})
        actual_score = (
            health_score.get("score", 50) if isinstance(health_score, dict) else 50
        )

        # ì ìˆ˜ â†’ ë“±ê¸‰ ë³€í™˜
        result["actual_level"] = self._score_to_grade_v2(actual_score)
        result["is_match"] = result["actual_level"] == result["expected_level"]

        # ì¸ì ‘ ë“±ê¸‰ í™•ì¸
        grade_order = [
            "warning",
            "caution",
            "moderate",
            "moderate_plus",
            "good",
            "optimal",
        ]
        try:
            expected_idx = grade_order.index(result["expected_level"])
            actual_idx = grade_order.index(result["actual_level"])
            result["is_adjacent"] = abs(expected_idx - actual_idx) <= 1
        except ValueError:
            pass

        return result

    # ============================================
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° Setup / Cleanup
    # ============================================

    def setup_test_data(self):
        """í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ê±´ê°• ë°ì´í„°ë¥¼ ChromaDBì— ì €ì¥"""
        if self.save_daily_summary is None:
            print("âš ï¸ vector_store ëª¨ë“ˆì´ ì—†ì–´ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¤ì • ë¶ˆê°€")
            return False

        print("\nğŸ“¦ í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ì„¤ì • ì¤‘...")
        today = datetime.now()
        self.test_data_ids = []

        for sample in SAMPLE_HEALTH_DATA:
            target_date = today + timedelta(days=sample["date_offset"])
            date_str = target_date.strftime("%Y-%m-%d")

            summary = {
                "created_at": f"{date_str}T12:00:00",
                "platform": "test_evaluation",
                "raw": sample["raw"],
            }

            try:
                result = self.save_daily_summary(
                    summary=summary, user_id=self.user_id, source="test_eval"
                )
                doc_id = result.get("document_id")
                if doc_id:
                    self.test_data_ids.append(doc_id)
                print(f"   âœ… {date_str} ë°ì´í„° ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                print(f"   âŒ {date_str} ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")

        print(f"ğŸ“¦ ì´ {len(self.test_data_ids)}ê°œ ìƒ˜í”Œ ë°ì´í„° ì €ì¥ ì™„ë£Œ\n")
        return True

    def cleanup_test_data(self):
        """í…ŒìŠ¤íŠ¸ í›„ ìƒ˜í”Œ ë°ì´í„° ì‚­ì œ"""
        if self.chroma_collection is None:
            return False

        if not self.test_data_ids:
            return True

        print("\nğŸ§¹ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬ ì¤‘...")
        try:
            self.chroma_collection.delete(ids=self.test_data_ids)
            print(f"ğŸ§¹ {len(self.test_data_ids)}ê°œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚­ì œ ì™„ë£Œ\n")
            self.test_data_ids = []
            return True
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨: {e}")
            return False

    # ============================================
    # ë©”ì¸ ì‹¤í–‰
    # ============================================

    def run_all(
        self, datasets_dir: str = "evaluation/datasets", cleanup: bool = False
    ) -> dict:
        """ëª¨ë“  ì„œë¹„ìŠ¤ í‰ê°€ ì‹¤í–‰"""
        stage = os.getenv("EVAL_MODE", "baseline")
        print("=" * 60)
        print(f"ğŸš€ {stage.upper()} í‰ê°€ ì‹œì‘ (v2.1)")
        print("=" * 60)

        try:
            self.setup_test_data()
            datasets_path = Path(datasets_dir)

            # 1. ê±´ê°• ë¶„ì„ í‰ê°€
            health_path = datasets_path / "health_data.json"
            if health_path.exists():
                print("\nğŸ“Š ê±´ê°• ë¶„ì„ í‰ê°€ ì¤‘...")
                self.results["health"] = self._run_health_evaluation(health_path)
                print(f"   ì™„ë£Œ: {len(self.results['health'])}ê±´")

            # 2. ìš´ë™ ë¶„ì„ í‰ê°€
            exercise_path = datasets_path / "exercise_data.json"
            if exercise_path.exists():
                print("\nğŸƒ ìš´ë™ ë¶„ì„ í‰ê°€ ì¤‘...")
                self.results["exercise"] = self._run_exercise_evaluation(exercise_path)
                print(f"   ì™„ë£Œ: {len(self.results['exercise'])}ê±´")

            # 3. ì±—ë´‡ í‰ê°€
            chat_path = datasets_path / "chat_queries.json"
            if chat_path.exists():
                print("\nğŸ’¬ ì±—ë´‡ í‰ê°€ ì¤‘...")
                self.results["chat"] = self._run_chat_evaluation(chat_path)
                print(f"   ì™„ë£Œ: {len(self.results['chat'])}ê±´")

            self.summary = self._generate_summary()

        finally:
            # âœ… ì—¬ê¸°ì„œ ë¬´ì¡°ê±´ ì €ì¥
            print("\nğŸ’¾ [AUTO SAVE] ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘...")
            self.summary = self._generate_summary()
            self.save_results()

            if cleanup:
                self.cleanup_test_data()

        return {
            "results": self.results,
            "summary": self.summary,
        }

    # ============================================
    # ê±´ê°• ë¶„ì„ í‰ê°€
    # ============================================

    def _run_health_evaluation(self, dataset_path: Path) -> list:
        """ê±´ê°• ë¶„ì„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        with open(dataset_path, "r", encoding="utf-8") as f:
            dataset = json.load(f)

        results = []
        for test_case in dataset.get("test_cases", []):
            result = self._evaluate_health_analysis(test_case)
            results.append(result)

        return results

    def _evaluate_health_analysis(self, test_case: dict) -> dict:
        """ë‹¨ì¼ ê±´ê°• ë¶„ì„ í‰ê°€"""
        result = {
            "id": test_case["id"],
            "scenario": test_case.get("scenario", ""),
            "difficulty": test_case.get("difficulty", "medium"),
            "input_data": test_case["input_data"],
            "expected": test_case["expected"],
            "responses": [],
            "times": [],
            "scores": {},
        }

        input_data = test_case["input_data"]
        expected = test_case["expected"]

        # ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰
        for _ in range(EVALUATION_ROUNDS):
            response, elapsed = self._call_health_interpreter(input_data)
            result["responses"].append(response)
            result["times"].append(elapsed)

        first_response = result["responses"][0]

        # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if isinstance(first_response, dict):
            response_text = first_response.get("llm_analysis", "")
            if not response_text:
                response_text = json.dumps(first_response, ensure_ascii=False)
        else:
            response_text = str(first_response)

        # === ê¸°ì¡´ í‰ê°€ ì§€í‘œ ===
        expected_keywords = expected.get("keywords", [])

        # === ìƒˆ í‰ê°€ ì§€í‘œ (v2.1) ===
        structure_check = self._check_structure_match(response_text, expected)
        citation_strict_check = self._check_citation_strict(response_text, expected)
        concept_check = self._check_concept_application(response_text, expected)
        length_check = self._check_length_appropriate(response_text, expected)
        grade_check = self._check_condition_grade_accuracy(first_response, expected)

        result["scores"] = {
            # ê¸°ì¡´ ì§€í‘œ
            "accuracy": self._calculate_health_accuracy_v2(first_response, expected),
            "keyword_match": ResponseQualityMetrics.keyword_match_score(
                response_text, expected_keywords
            ),
            "consistency": self._calculate_dict_consistency(result["responses"]),
            "avg_time": PerformanceMetrics.calculate_stats(result["times"])["avg"],
            "avg_tokens": PerformanceMetrics.estimate_tokens(response_text),
            # ìƒˆ ì§€í‘œ (v2.1 - ë¶„ë¦¬)
            "structure_match": structure_check,
            "citation_strict": citation_strict_check,
            "concept_application": concept_check,
            "length": length_check,
            "grade_accuracy": grade_check,
            # ì¢…í•© ì ìˆ˜
            "structure_score": sum(structure_check.values()) / 3 * 100,
            "citation_strict_score": citation_strict_check["citation_strict_score"]
            * 100,
            "concept_score": concept_check["concept_score"] * 100,
            "length_score": length_check["length_score"] * 100,
            "grade_match": grade_check["is_match"],
        }

        return result

    def _call_health_interpreter(self, input_data: dict) -> tuple:
        start = datetime.now()

        try:
            # EVAL_MODEì— ë”°ë¼ interpret_health_data ë‚´ë¶€ì—ì„œ ë¶„ê¸°
            result = self.interpret_health_data(input_data)

            elapsed = (datetime.now() - start).total_seconds()
            return result, elapsed

        except Exception as e:
            elapsed = (datetime.now() - start).total_seconds()
            return {"error": str(e)}, elapsed

    def _calculate_health_accuracy_v2(self, response: dict, expected: dict) -> float:
        """ê±´ê°• ë¶„ì„ ì •í™•ë„ ê³„ì‚° (v2 - 6ë“±ê¸‰)"""
        if isinstance(response, dict) and "error" in response:
            return 0.0

        score = 0
        total = 0

        # 1. ì»¨ë””ì…˜ ë“±ê¸‰ ë§¤ì¹­ (40ì )
        total += 40
        expected_level = expected.get("condition_level", "")
        if isinstance(response, dict):
            health_score = response.get("health_score", {})
            actual_score = (
                health_score.get("score", 50) if isinstance(health_score, dict) else 50
            )
            actual_level = self._score_to_grade_v2(actual_score)

            if actual_level == expected_level:
                score += 40
            elif self._is_adjacent_grade(actual_level, expected_level):
                score += 25  # ì¸ì ‘ ë“±ê¸‰

        # 2. ìš´ë™ ê°•ë„ ê¶Œì¥ ë§¤ì¹­ (30ì )
        total += 30
        expected_exercise = expected.get("exercise_recommendation", "")
        if isinstance(response, dict):
            exercise_rec = response.get("exercise_recommendation", {})
            if isinstance(exercise_rec, dict):
                rec_level = exercise_rec.get("recommended_level", "")
                if self._match_exercise_recommendation(rec_level, expected_exercise):
                    score += 30

        # 3. í‚¤ì›Œë“œ í¬í•¨ (30ì )
        total += 30
        expected_keywords = expected.get("keywords", [])
        if expected_keywords:
            response_text = (
                json.dumps(response, ensure_ascii=False)
                if isinstance(response, dict)
                else str(response)
            )
            matched = sum(1 for kw in expected_keywords if kw in response_text)
            score += (matched / len(expected_keywords)) * 30

        return round((score / total) * 100, 1) if total > 0 else 0.0

    def _is_adjacent_grade(self, grade1: str, grade2: str) -> bool:
        """ì¸ì ‘ ë“±ê¸‰ í™•ì¸"""
        grade_order = [
            "warning",
            "caution",
            "moderate",
            "moderate_plus",
            "good",
            "optimal",
        ]
        try:
            idx1 = grade_order.index(grade1)
            idx2 = grade_order.index(grade2)
            return abs(idx1 - idx2) <= 1
        except ValueError:
            return False

    def _match_exercise_recommendation(self, rec_level: str, expected: str) -> bool:
        """ìš´ë™ ê¶Œì¥ ë§¤ì¹­"""
        if "ê³ ê°•ë„" in expected and rec_level in ["ê³ ", "ìƒ", "ê³ ê°•ë„"]:
            return True
        if "ì¤‘ê°•ë„" in expected and rec_level in ["ì¤‘", "ì¤‘ê°•ë„"]:
            return True
        if "ì €ê°•ë„" in expected and rec_level in ["í•˜", "ì €", "ì €ê°•ë„"]:
            return True
        if "íœ´ì‹" in expected and rec_level in ["íœ´ì‹", "í•˜"]:
            return True
        return False

    def _calculate_dict_consistency(self, responses: list) -> float:
        """ë”•ì…”ë„ˆë¦¬ ì‘ë‹µ ì¼ê´€ì„±"""
        if len(responses) < 2:
            return 1.0

        first = responses[0]
        if not isinstance(first, dict):
            return ResponseQualityMetrics.consistency_score([str(r) for r in responses])

        consistent_count = 0
        for resp in responses[1:]:
            if isinstance(resp, dict):
                first_score = (
                    first.get("health_score", {}).get("score", 0)
                    if isinstance(first.get("health_score"), dict)
                    else 0
                )
                resp_score = (
                    resp.get("health_score", {}).get("score", 0)
                    if isinstance(resp.get("health_score"), dict)
                    else 0
                )
                if abs(first_score - resp_score) <= 5:
                    consistent_count += 1

        return consistent_count / (len(responses) - 1)

    # ============================================
    # ìš´ë™ ë¶„ì„ í‰ê°€
    # ============================================

    def _run_exercise_evaluation(self, dataset_path: Path) -> list:
        """ìš´ë™ ë¶„ì„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        with open(dataset_path, "r", encoding="utf-8") as f:
            dataset = json.load(f)

        results = []
        for test_case in dataset.get("test_cases", []):
            result = self._evaluate_exercise_analysis(test_case)
            results.append(result)

        return results

    def _evaluate_exercise_analysis(self, test_case: dict) -> dict:
        """ë‹¨ì¼ ìš´ë™ ë¶„ì„ í‰ê°€"""
        result = {
            "id": test_case["id"],
            "scenario": test_case.get("scenario", ""),
            "difficulty": test_case.get("difficulty", "medium"),
            "input_data": test_case["input_data"],
            "expected": test_case["expected"],
            "responses": [],
            "times": [],
            "scores": {},
        }

        input_data = test_case["input_data"]
        routine = input_data.get("routine", {})
        expected = test_case["expected"]

        # ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰
        for _ in range(EVALUATION_ROUNDS):
            response, elapsed = self._call_llm_analysis(input_data, routine)
            result["responses"].append(response)
            result["times"].append(elapsed)

        first_response = result["responses"][0]

        # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if isinstance(first_response, dict):
            response_text = first_response.get("analysis", "") or first_response.get(
                "llm_analysis", ""
            )
            if not response_text:
                response_text = json.dumps(first_response, ensure_ascii=False)
        else:
            response_text = str(first_response)

        # ì¹´ë³´ë„¨ ê³µì‹ í™•ì¸ (ì €ìëª…ë§Œ)
        karvonen_cited = any(
            kw in response_text for kw in PROFESSIONAL_REFERENCES_STRICT["karvonen"]
        )

        # ì¹´ë³´ë„¨ ê°œë… ì ìš© í™•ì¸
        karvonen_concept_applied = any(
            kw in response_text for kw in CONCEPT_KEYWORDS["karvonen_concept"]
        )

        # ìƒˆ í‰ê°€ ì§€í‘œ
        structure_check = self._check_structure_match(response_text, expected)
        length_check = self._check_length_appropriate(response_text, expected)

        result["scores"] = {
            "accuracy": self._calculate_exercise_accuracy(first_response, expected),
            "keyword_match": ResponseQualityMetrics.keyword_match_score(
                response_text, expected.get("keywords", [])
            ),
            "consistency": self._calculate_dict_consistency(result["responses"]),
            "avg_time": PerformanceMetrics.calculate_stats(result["times"])["avg"],
            "avg_tokens": PerformanceMetrics.estimate_tokens(response_text),
            # ìƒˆ ì§€í‘œ (v2.1 - ë¶„ë¦¬)
            "karvonen_cited": karvonen_cited,
            "karvonen_concept_applied": karvonen_concept_applied,
            "structure_match": structure_check,
            "length": length_check,
            "structure_score": sum(structure_check.values()) / 3 * 100,
            "length_score": length_check["length_score"] * 100,
        }

        return result

    def _call_llm_analysis(self, input_data: dict, routine: dict) -> tuple:
        start = datetime.now()

        try:
            # EVAL_MODEì— ë”°ë¼ run_llm_analysis ë‚´ë¶€ì—ì„œ ë¶„ê¸°
            summary = {"raw": input_data}
            result = self.run_llm_analysis(
                summary=summary,
                user_id=self.user_id,
                difficulty_level=routine.get("difficulty", "medium"),
                duration_min=routine.get("duration_min", 30),
            )

            elapsed = (datetime.now() - start).total_seconds()
            return result, elapsed

        except Exception as e:
            elapsed = (datetime.now() - start).total_seconds()
            return {"error": str(e)}, elapsed

    def _calculate_exercise_accuracy(self, response: dict, expected: dict) -> float:
        """ìš´ë™ ë¶„ì„ ì •í™•ë„ ê³„ì‚°"""
        if isinstance(response, dict) and "error" in response:
            return 0.0

        score = 0
        total = 100

        # 1. ì í•©ë„ í‰ê°€ í¬í•¨ (30ì )
        response_text = (
            json.dumps(response, ensure_ascii=False)
            if isinstance(response, dict)
            else str(response)
        )
        if any(kw in response_text for kw in ["ì í•©", "ë¶€ì í•©", "ê¶Œì¥", "ì£¼ì˜"]):
            score += 30

        # 2. ê¶Œì¥ ê°•ë„ ë§¤ì¹­ (40ì )
        expected_intensity = expected.get("recommended_intensity", "")
        if expected_intensity in response_text:
            score += 40
        elif any(kw in response_text for kw in ["ê°•ë„", "intensity"]):
            score += 20

        # 3. í‚¤ì›Œë“œ í¬í•¨ (30ì )
        expected_keywords = expected.get("keywords", [])
        if expected_keywords:
            matched = sum(1 for kw in expected_keywords if kw in response_text)
            score += (matched / len(expected_keywords)) * 30

        return round(score, 1)

    # ============================================
    # ì±—ë´‡ í‰ê°€
    # ============================================

    def _run_chat_evaluation(self, chat_path: Path) -> list:
        with open(chat_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        results = []

        for case in data["test_cases"]:
            character = case["input_data"].get("character", "devil_coach")

            # âœ… ììœ í˜• ì±—ë´‡ë§Œ í‰ê°€
            if character in ["devil_coach", "angel_coach", "booster_coach"]:
                result = self._evaluate_chat(case)
                results.append(result)

        return results

    def _evaluate_chat(self, test_case: dict) -> dict:
        """ë‹¨ì¼ ì±—ë´‡ í‰ê°€"""
        result = {
            "id": test_case["id"],
            "category": test_case.get("category", ""),
            "difficulty": test_case.get("difficulty", "medium"),
            "input_data": test_case["input_data"],
            "expected": test_case["expected"],
            "responses": [],
            "times": [],
            "scores": {},
        }

        input_data = test_case["input_data"]
        expected = test_case["expected"]
        message = input_data.get("message", "")
        character = input_data.get("character", "devil_coach")

        # ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰
        for _ in range(EVALUATION_ROUNDS):
            response, elapsed = self._call_chat_api(message, character)
            result["responses"].append(response)
            result["times"].append(elapsed)

        first_response = result["responses"][0]
        response_text = (
            first_response if isinstance(first_response, str) else str(first_response)
        )

        # ìƒˆ í‰ê°€ ì§€í‘œ
        citation_strict_check = self._check_citation_strict(response_text, expected)
        concept_check = self._check_concept_application(response_text, expected)
        length_check = self._check_length_appropriate(response_text, expected)

        result["scores"] = {
            "accuracy": self._calculate_chat_accuracy(
                response_text, expected, character
            ),
            "keyword_match": ResponseQualityMetrics.keyword_match_score(
                response_text, expected.get("keywords", [])
            ),
            "consistency": ResponseQualityMetrics.consistency_score(
                result["responses"]
            ),
            "avg_time": PerformanceMetrics.calculate_stats(result["times"])["avg"],
            "avg_tokens": PerformanceMetrics.estimate_tokens(response_text),
            # ìƒˆ ì§€í‘œ (v2.1)
            "citation_strict": citation_strict_check,
            "concept_application": concept_check,
            "length": length_check,
            "citation_strict_score": citation_strict_check["citation_strict_score"]
            * 100,
            "concept_score": concept_check["concept_score"] * 100,
            "length_score": length_check["length_score"] * 100,
        }

        return result

    def _call_chat_api(self, message: str, character: str) -> tuple:
        """ì±—ë´‡ API í˜¸ì¶œ"""
        start = datetime.now()

        try:
            response = requests.post(
                f"{self.base_url}/api/chat",
                json={
                    "user_id": self.user_id,
                    "message": message,
                    "character": character,
                },
                timeout=30,
            )
            elapsed = (datetime.now() - start).total_seconds()

            if response.status_code == 200:
                data = response.json()
                return data.get("response", ""), elapsed
            else:
                return f"Error: {response.status_code}", elapsed
        except Exception as e:
            elapsed = (datetime.now() - start).total_seconds()
            return f"Error: {str(e)}", elapsed

    def _calculate_chat_accuracy(
        self, response: str, expected: dict, character: str
    ) -> float:
        """ì±—ë´‡ ì •í™•ë„ ê³„ì‚°"""
        score = 0
        total = 100

        # í‚¤ì›Œë“œ ë§¤ì¹­ (50ì )
        expected_keywords = expected.get("keywords", [])
        if expected_keywords:
            matched = sum(1 for kw in expected_keywords if kw in response)
            score += (matched / len(expected_keywords)) * 50

        # í†¤ ë§¤ì¹­ (30ì )
        if self._check_tone_match(response, expected, character):
            score += 30

        # ì‘ë‹µ ê¸¸ì´ ì ì ˆì„± (20ì )
        length_score = ResponseQualityMetrics.response_length_score(response)
        score += length_score * 20

        return round(score, 1)

    def _check_tone_match(self, response: str, expected: dict, character: str) -> bool:
        """í˜ë¥´ì†Œë‚˜ í†¤ ë§¤ì¹­"""
        tone_keywords = {
            "devil_coach": {"tough_love": ["í•´ì•¼ì§€", "ë³€ëª…", "í•‘ê³„", "ë‹¹ì¥", "ê²Œìœ¼ë¦„"]},
            "angel_coach": {"supportive": ["ì˜í–ˆì–´", "ëŒ€ë‹¨í•´", "ë©‹ì ¸", "ìµœê³ ", "í›Œë¥­"]},
            "booster_coach": {
                "encouraging": ["í•  ìˆ˜ ìˆì–´", "íŒŒì´íŒ…", "ë¯¿ì–´", "ê´œì°®ì•„", "í˜ë‚´"]
            },
        }

        expected_tone = expected.get("tone", "")
        character_tones = tone_keywords.get(character, {})
        keywords = character_tones.get(expected_tone, [])

        if not keywords:
            return True

        return any(kw in response for kw in keywords)

    # ============================================
    # ìš”ì•½ ë° ì €ì¥
    # ============================================

    def _generate_summary(self) -> dict:
        """ìš”ì•½ í†µê³„ ìƒì„± (v2.1 ì§€í‘œ í¬í•¨)"""
        summary = {
            "timestamp": datetime.now().isoformat(),
            "stage": "baseline",
            "version": "v2.1",
            "total_queries": 0,
            "by_service": {},
        }

        for service, results in self.results.items():
            if not results:
                continue

            scored_results = [r for r in results if "scores" in r]

            service_summary = {
                "count": len(results),
                "avg_accuracy": 0,
                "avg_keyword_match": 0,
                "avg_consistency": 0,
                "avg_time": 0,
                "avg_tokens": 0,
                "avg_structure_score": 0,
                "avg_citation_strict_score": 0,
                "avg_concept_score": 0,
                "avg_length_score": 0,
            }

            if scored_results:
                service_summary["avg_accuracy"] = round(
                    sum(r["scores"]["accuracy"] for r in scored_results)
                    / len(scored_results),
                    2,
                )
                service_summary["avg_keyword_match"] = round(
                    sum(r["scores"]["keyword_match"] for r in scored_results)
                    / len(scored_results),
                    4,
                )
                service_summary["avg_consistency"] = round(
                    sum(r["scores"]["consistency"] for r in scored_results)
                    / len(scored_results),
                    4,
                )
                service_summary["avg_time"] = round(
                    sum(r["scores"]["avg_time"] for r in scored_results)
                    / len(scored_results),
                    4,
                )
                service_summary["avg_tokens"] = round(
                    sum(r["scores"]["avg_tokens"] for r in scored_results)
                    / len(scored_results),
                    0,
                )

                service_summary["avg_structure_score"] = round(
                    sum(r["scores"].get("structure_score", 0) for r in scored_results)
                    / len(scored_results),
                    2,
                )
                service_summary["avg_citation_strict_score"] = round(
                    sum(
                        r["scores"].get("citation_strict_score", 0)
                        for r in scored_results
                    )
                    / len(scored_results),
                    2,
                )
                service_summary["avg_concept_score"] = round(
                    sum(r["scores"].get("concept_score", 0) for r in scored_results)
                    / len(scored_results),
                    2,
                )
                service_summary["avg_length_score"] = round(
                    sum(r["scores"].get("length_score", 0) for r in scored_results)
                    / len(scored_results),
                    2,
                )

                if service == "health":
                    grade_matches = sum(
                        1
                        for r in scored_results
                        if r["scores"].get("grade_match", False)
                    )
                    service_summary["grade_accuracy"] = round(
                        grade_matches / len(scored_results) * 100, 2
                    )

                if service == "exercise":
                    karvonen_cited = sum(
                        1
                        for r in scored_results
                        if r["scores"].get("karvonen_cited", False)
                    )
                    karvonen_concept = sum(
                        1
                        for r in scored_results
                        if r["scores"].get("karvonen_concept_applied", False)
                    )
                    service_summary["karvonen_citation_rate"] = round(
                        karvonen_cited / len(scored_results) * 100, 2
                    )
                    service_summary["karvonen_concept_rate"] = round(
                        karvonen_concept / len(scored_results) * 100, 2
                    )

            summary["by_service"][service] = service_summary
            summary["total_queries"] += service_summary["count"]

        # ì „ì²´ í‰ê·  (scores ìˆëŠ” ê²ƒë§Œ)
        all_scored = [
            r
            for r in (
                self.results.get("health", [])
                + self.results.get("exercise", [])
                + self.results.get("chat", [])
            )
            if "scores" in r
        ]

        if all_scored:
            summary["overall"] = {
                "avg_accuracy": round(
                    sum(r["scores"]["accuracy"] for r in all_scored) / len(all_scored),
                    2,
                ),
                "avg_time": round(
                    sum(r["scores"]["avg_time"] for r in all_scored) / len(all_scored),
                    4,
                ),
                "avg_tokens": round(
                    sum(r["scores"]["avg_tokens"] for r in all_scored)
                    / len(all_scored),
                    0,
                ),
                "avg_structure_score": round(
                    sum(r["scores"].get("structure_score", 0) for r in all_scored)
                    / len(all_scored),
                    2,
                ),
                "avg_citation_strict_score": round(
                    sum(r["scores"].get("citation_strict_score", 0) for r in all_scored)
                    / len(all_scored),
                    2,
                ),
                "avg_concept_score": round(
                    sum(r["scores"].get("concept_score", 0) for r in all_scored)
                    / len(all_scored),
                    2,
                ),
            }

        return summary

    def save_results(self, output_dir: str = None) -> Path:
        """ê²°ê³¼ ì €ì¥"""
        if output_dir is None:
            output_dir = f"{RESULTS_DIR}/baseline"

        Path(output_dir).mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = Path(output_dir) / f"results_v2.1_{timestamp}.json"

        output_data = {
            "metadata": {
                "stage": "baseline",
                "version": "v2.1",
                "timestamp": datetime.now().isoformat(),
                "api_base_url": self.base_url,
                "evaluation_rounds": EVALUATION_ROUNDS,
                "condition_grades": "6ë“±ê¸‰ (A/B/C+/C/D/F)",
                "citation_method": "strict (ì €ìëª…ë§Œ) + concept (ê°œë… í‚¤ì›Œë“œ)",
            },
            "summary": self.summary,
            "results": self.results,
        }

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
        return output_path

    def print_summary(self):
        """ìš”ì•½ ì¶œë ¥ (v2.1 ì§€í‘œ í¬í•¨)"""
        print("\n" + "=" * 60)
        print("ğŸ“Š Baseline í‰ê°€ ìš”ì•½ (v2.1)")
        print("=" * 60)

        print(f"\nì´ í…ŒìŠ¤íŠ¸: {self.summary.get('total_queries', 0)}ê±´")

        for service, stats in self.summary.get("by_service", {}).items():
            service_name = {
                "health": "ê±´ê°• ë¶„ì„",
                "exercise": "ìš´ë™ ë¶„ì„",
                "chat": "ì±—ë´‡",
            }.get(service, service)

            print(f"\n[{service_name}] ({stats['count']}ê±´)")
            print(f"   ì •í™•ë„: {stats['avg_accuracy']:.1f}%")
            print(f"   í‚¤ì›Œë“œ ë§¤ì¹­: {stats['avg_keyword_match']:.2f}")
            print(f"   ì¼ê´€ì„±: {stats['avg_consistency']:.2f}")
            print(f"   ì‘ë‹µ ì‹œê°„: {stats['avg_time']:.2f}ì´ˆ")

            # v2.1 ì§€í‘œ (ë¶„ë¦¬)
            print(f"   [v2.1 ì§€í‘œ]")
            print(f"   ì‘ë‹µ êµ¬ì¡° ì ìˆ˜: {stats['avg_structure_score']:.1f}%")
            print(
                f"   ğŸ“š ë…¼ë¬¸ ì¸ìš©ìœ¨ (ì €ìëª…): {stats['avg_citation_strict_score']:.1f}%"
            )
            print(f"   ğŸ’¡ ê°œë… ì ìš©ìœ¨: {stats['avg_concept_score']:.1f}%")
            print(f"   ê¸¸ì´ ì ì ˆì„±: {stats['avg_length_score']:.1f}%")

            if "grade_accuracy" in stats:
                print(f"   ë“±ê¸‰ ì •í™•ë„: {stats['grade_accuracy']:.1f}%")
            if "karvonen_citation_rate" in stats:
                print(
                    f"   ğŸ“š ì¹´ë³´ë„¨ ì¸ìš©ìœ¨ (ì €ìëª…): {stats['karvonen_citation_rate']:.1f}%"
                )
                print(f"   ğŸ’¡ ì¹´ë³´ë„¨ ê°œë…ìœ¨: {stats['karvonen_concept_rate']:.1f}%")

        if "overall" in self.summary:
            print(f"\n[ì „ì²´ í‰ê· ]")
            print(f"   ì •í™•ë„: {self.summary['overall']['avg_accuracy']:.1f}%")
            print(f"   ì‘ë‹µ ì‹œê°„: {self.summary['overall']['avg_time']:.2f}ì´ˆ")
            print(
                f"   ì‘ë‹µ êµ¬ì¡° ì ìˆ˜: {self.summary['overall']['avg_structure_score']:.1f}%"
            )
            print(
                f"   ğŸ“š ë…¼ë¬¸ ì¸ìš©ìœ¨ (ì €ìëª…): {self.summary['overall']['avg_citation_strict_score']:.1f}%"
            )
            print(
                f"   ğŸ’¡ ê°œë… ì ìš©ìœ¨: {self.summary['overall']['avg_concept_score']:.1f}%"
            )


if __name__ == "__main__":
    runner = BaselineRunner()
    runner.run_all()
    runner.print_summary()
