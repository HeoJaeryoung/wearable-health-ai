"""
LLM Fine-tuning í•™ìŠµ ë°ì´í„° í†µí•© ë° ë¶„í•  ìŠ¤í¬ë¦½íŠ¸ v2

4ê°œ ì¹´í…Œê³ ë¦¬ í†µí•©:
- ê±´ê°• ë°ì´í„° í•´ì„: 400ê±´
- ìš´ë™ ë¶„ì„: 300ê±´
- ì½”ì¹­ ì±—ë´‡: 500ê±´
- íŒë‹¨ íŒ¨í„´: 300ê±´
= ì´ 1,500ê±´

ë¶„í• : Train 80% (1,200ê±´) / Valid 10% (150ê±´) / Test 10% (150ê±´)
"""

import json
import random
import os
from datetime import datetime
from pathlib import Path


def load_jsonl(filepath: str) -> list:
    """JSONL íŒŒì¼ ë¡œë“œ"""
    data = []
    with open(filepath, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line.strip()))
    return data


def save_jsonl(data: list, filepath: str):
    """JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
    with open(filepath, "w", encoding="utf-8") as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")


def validate_data(data: list) -> dict:
    """ë°ì´í„° ìœ íš¨ì„± ê²€ì¦"""
    stats = {"total": len(data), "valid": 0, "invalid": 0, "errors": []}

    for i, item in enumerate(data):
        try:
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            assert "messages" in item, "messages í•„ë“œ ì—†ìŒ"
            assert len(item["messages"]) >= 2, "messages ê¸¸ì´ ë¶€ì¡±"

            # ì—­í•  í™•ì¸
            roles = [m.get("role") for m in item["messages"]]
            assert "system" in roles, "system ì—­í•  ì—†ìŒ"
            assert "user" in roles, "user ì—­í•  ì—†ìŒ"
            assert "assistant" in roles, "assistant ì—­í•  ì—†ìŒ"

            # ë‚´ìš© í™•ì¸
            for msg in item["messages"]:
                assert msg.get("content"), "ë¹ˆ content"

            stats["valid"] += 1

        except AssertionError as e:
            stats["invalid"] += 1
            stats["errors"].append(f"[{i}] {str(e)}")

    return stats


def merge_and_split(
    train_ratio: float = 0.8, valid_ratio: float = 0.1, test_ratio: float = 0.1
):
    """ë°ì´í„° í†µí•© ë° ë¶„í•  (Train/Valid/Test)"""

    print("=" * 60)
    print("ğŸš€ LLM Fine-tuning ë°ì´í„° í†µí•© v2")
    print("=" * 60)

    # ë¹„ìœ¨ ê²€ì¦
    assert (
        abs(train_ratio + valid_ratio + test_ratio - 1.0) < 0.001
    ), "ë¹„ìœ¨ í•©ì´ 1.0ì´ì–´ì•¼ í•©ë‹ˆë‹¤"

    data_dir = Path(__file__).parent.parent / "data"

    # íŒŒì¼ ëª©ë¡
    files = {
        "ê±´ê°• ë¶„ì„": "health_interpretation_data_v2.jsonl",
        "ìš´ë™ ë¶„ì„": "exercise_analysis_data_v2.jsonl",
        "ì½”ì¹­ ì±—ë´‡": "coaching_chat_data_v2.jsonl",
        "íŒë‹¨ íŒ¨í„´": "decision_pattern_data_v2.jsonl",
    }

    all_data = []
    category_counts = {}

    print("\nğŸ“‚ íŒŒì¼ ë¡œë“œ ì¤‘...")
    for category, filename in files.items():
        filepath = data_dir / filename
        if filepath.exists():
            data = load_jsonl(filepath)
            category_counts[category] = len(data)
            all_data.extend(data)
            print(f"   âœ… {category}: {len(data)}ê±´")
        else:
            print(f"   âŒ {category}: íŒŒì¼ ì—†ìŒ ({filename})")
            category_counts[category] = 0

    print(f"\nğŸ“Š ì´ ë°ì´í„°: {len(all_data)}ê±´")

    # ìœ íš¨ì„± ê²€ì¦
    print("\nğŸ” ë°ì´í„° ìœ íš¨ì„± ê²€ì¦ ì¤‘...")
    validation = validate_data(all_data)
    print(f"   - ìœ íš¨: {validation['valid']}ê±´")
    print(f"   - ë¬´íš¨: {validation['invalid']}ê±´")

    if validation["invalid"] > 0:
        print(f"   âš ï¸ ì˜¤ë¥˜ ìƒ˜í”Œ: {validation['errors'][:3]}")

    # ì„ê¸°
    random.seed(42)
    random.shuffle(all_data)

    # ë¶„í•  (Train / Valid / Test)
    total = len(all_data)
    train_end = int(total * train_ratio)
    valid_end = int(total * (train_ratio + valid_ratio))

    train_data = all_data[:train_end]
    valid_data = all_data[train_end:valid_end]
    test_data = all_data[valid_end:]

    print(
        f"\nğŸ“‚ ë°ì´í„° ë¶„í•  (Train {int(train_ratio*100)}% / Valid {int(valid_ratio*100)}% / Test {int(test_ratio*100)}%)"
    )
    print(f"   - Train: {len(train_data)}ê±´ (í•™ìŠµìš©)")
    print(f"   - Valid: {len(valid_data)}ê±´ (í•™ìŠµ ì¤‘ ê²€ì¦)")
    print(f"   - Test:  {len(test_data)}ê±´ (í•™ìŠµ í›„ ëª¨ë¸ í‰ê°€)")

    # ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d")

    train_file = data_dir / f"train_{timestamp}.jsonl"
    valid_file = data_dir / f"valid_{timestamp}.jsonl"
    test_file = data_dir / f"test_model_{timestamp}.jsonl"

    save_jsonl(train_data, train_file)
    save_jsonl(valid_data, valid_file)
    save_jsonl(test_data, test_file)

    print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ:")
    print(f"   - {train_file.name} ({len(train_data)}ê±´)")
    print(f"   - {valid_file.name} ({len(valid_data)}ê±´)")
    print(f"   - {test_file.name} ({len(test_data)}ê±´)")

    # í†µê³„ ì €ì¥
    stats = {
        "generated_at": datetime.now().isoformat(),
        "total_count": len(all_data),
        "train_count": len(train_data),
        "valid_count": len(valid_data),
        "test_count": len(test_data),
        "ratios": {"train": train_ratio, "valid": valid_ratio, "test": test_ratio},
        "categories": category_counts,
        "validation": {"valid": validation["valid"], "invalid": validation["invalid"]},
    }

    stats_file = data_dir / f"stats_{timestamp}.json"
    with open(stats_file, "w", encoding="utf-8") as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)

    print(f"   - {stats_file.name} (í†µê³„)")

    return train_file, valid_file, test_file, stats


def print_sample(filepath: str, count: int = 2):
    """ìƒ˜í”Œ ì¶œë ¥"""
    data = load_jsonl(filepath)

    print(f"\nğŸ“ ìƒ˜í”Œ ({filepath}):")
    print("-" * 40)

    for i, item in enumerate(data[:count]):
        print(f"\n[ìƒ˜í”Œ {i+1}]")
        print(f"User: {item['messages'][1]['content'][:100]}...")
        print(f"Assistant: {item['messages'][2]['content'][:150]}...")


# ============================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================

if __name__ == "__main__":
    train_file, valid_file, test_file, stats = merge_and_split(
        train_ratio=0.8, valid_ratio=0.1, test_ratio=0.1
    )

    print("\n" + "=" * 60)
    print("âœ… ì™„ë£Œ!")
    print("=" * 60)

    print("\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
    for category, count in stats["categories"].items():
        ratio = count / stats["total_count"] * 100 if stats["total_count"] > 0 else 0
        print(f"   - {category}: {count}ê±´ ({ratio:.1f}%)")

    print("\nğŸ“ íŒŒì¼ ìš©ë„:")
    print(f"   - Train ({train_file.name}): Azure Fine-tuning í•™ìŠµ")
    print(f"   - Valid ({valid_file.name}): Azure Fine-tuning ê²€ì¦ (Loss ëª¨ë‹ˆí„°ë§)")
    print(f"   - Test ({test_file.name}): í•™ìŠµ í›„ ëª¨ë¸ í‰ê°€ (ì •ë‹µ ë¹„êµ)")

    print("\nâš™ï¸ ê¶Œì¥ Fine-tuning ì„¤ì •:")
    print("   - Base Model: Llama 3.1 8B Instruct")
    print("   - Epochs: 3")
    print("   - Batch Size: 4")
    print("   - Learning Rate: 2e-4")
    print("   - LoRA Rank: 16")

    print("\nğŸ“‹ í‰ê°€ ì²´ê³„:")
    print("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("   â”‚  1. ëª¨ë¸ í‰ê°€ (test_model_*.jsonl)          â”‚")
    print("   â”‚     - Fine-tuned ëª¨ë¸ì˜ í•™ìŠµ í’ˆì§ˆ ì¸¡ì •      â”‚")
    print("   â”‚     - ì •ë‹µ(assistant)ê³¼ ì¶œë ¥ ë¹„êµ           â”‚")
    print("   â”‚                                             â”‚")
    print("   â”‚  2. ì„œë¹„ìŠ¤ í‰ê°€ (evaluation/datasets/)      â”‚")
    print("   â”‚     - Baseline â†’ LangChain â†’ Fine-tuned    â”‚")
    print("   â”‚     - 3ë‹¨ê³„ ì¶œë ¥ í’ˆì§ˆ ë¹„êµ                  â”‚")
    print("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
