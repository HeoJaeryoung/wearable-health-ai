"""
Baseline í‰ê°€ ì‹¤í–‰ê¸°
3ê°œ ì„œë¹„ìŠ¤(ê±´ê°•ë¶„ì„, ìš´ë™ì¶”ì²œ, ì±—ë´‡) ëª¨ë‘ í‰ê°€

ì„œë¹„ìŠ¤ë³„ í˜¸ì¶œ ë°©ì‹:
- ê±´ê°• ë¶„ì„: interpret_health_data() ì§ì ‘ í˜¸ì¶œ (ìƒì²´ ë°ì´í„° ì…ë ¥)
- ìš´ë™ ì¶”ì²œ: run_llm_analysis() ì§ì ‘ í˜¸ì¶œ (ìƒì²´ ë°ì´í„° + ì˜µì…˜ ì…ë ¥)
- ì±—ë´‡: /api/chat API í˜¸ì¶œ (ì§ˆë¬¸ í…ìŠ¤íŠ¸ ì…ë ¥) + RAG ê²€ìƒ‰

ì±—ë´‡ RAG í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´:
- í…ŒìŠ¤íŠ¸ ì‹œì‘ ì „ ìƒ˜í”Œ ê±´ê°• ë°ì´í„°ë¥¼ ChromaDBì— ì €ì¥
- í…ŒìŠ¤íŠ¸ ì¢…ë£Œ í›„ ìƒ˜í”Œ ë°ì´í„° ì‚­ì œ
"""

import json
import requests
from pathlib import Path
from datetime import datetime, timedelta
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from evaluation.config import API_BASE_URL, TEST_USER_ID, EVALUATION_ROUNDS, RESULTS_DIR
from evaluation.metrics.response_quality import ResponseQualityMetrics
from evaluation.metrics.performance import PerformanceMetrics
from evaluation.metrics.rag_quality import RAGQualityMetrics


# ============================================
# í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ê±´ê°• ë°ì´í„° (7ì¼ì¹˜)
# ============================================
SAMPLE_HEALTH_DATA = [
    {
        "date_offset": 0,  # ì˜¤ëŠ˜
        "raw": {
            "heart_rate": 72,
            "resting_heart_rate": 62,
            "sleep_hr": 7.5,
            "steps": 8500,
            "distance_km": 6.2,
            "active_calories": 380,
            "oxygen_saturation": 97,
            "weight": 72.0,
            "bmi": 23.5,
        },
    },
    {
        "date_offset": -1,  # ì–´ì œ
        "raw": {
            "heart_rate": 68,
            "resting_heart_rate": 60,
            "sleep_hr": 8.0,
            "steps": 10200,
            "distance_km": 7.8,
            "active_calories": 450,
            "oxygen_saturation": 98,
            "weight": 71.8,
            "bmi": 23.4,
        },
    },
    {
        "date_offset": -2,
        "raw": {
            "heart_rate": 75,
            "resting_heart_rate": 65,
            "sleep_hr": 6.5,
            "steps": 6000,
            "distance_km": 4.5,
            "active_calories": 280,
            "oxygen_saturation": 96,
            "weight": 72.2,
            "bmi": 23.6,
        },
    },
    {
        "date_offset": -3,
        "raw": {
            "heart_rate": 70,
            "resting_heart_rate": 61,
            "sleep_hr": 7.8,
            "steps": 9000,
            "distance_km": 6.8,
            "active_calories": 400,
            "oxygen_saturation": 97,
            "weight": 72.0,
            "bmi": 23.5,
        },
    },
    {
        "date_offset": -4,
        "raw": {
            "heart_rate": 78,
            "resting_heart_rate": 68,
            "sleep_hr": 5.5,
            "steps": 4500,
            "distance_km": 3.2,
            "active_calories": 200,
            "oxygen_saturation": 95,
            "weight": 72.5,
            "bmi": 23.7,
        },
    },
    {
        "date_offset": -5,
        "raw": {
            "heart_rate": 65,
            "resting_heart_rate": 58,
            "sleep_hr": 8.5,
            "steps": 12000,
            "distance_km": 9.0,
            "active_calories": 520,
            "oxygen_saturation": 98,
            "weight": 71.5,
            "bmi": 23.3,
        },
    },
    {
        "date_offset": -6,
        "raw": {
            "heart_rate": 73,
            "resting_heart_rate": 63,
            "sleep_hr": 7.0,
            "steps": 7500,
            "distance_km": 5.5,
            "active_calories": 340,
            "oxygen_saturation": 97,
            "weight": 72.0,
            "bmi": 23.5,
        },
    },
]


class BaselineRunner:

    def __init__(self):
        self.base_url = API_BASE_URL
        self.user_id = TEST_USER_ID
        self.results = {"health": [], "exercise": [], "chat": []}
        self.summary = {}
        self.test_data_ids = []  # ì €ì¥ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° ID ì¶”ì 

        # ì„œë¹„ìŠ¤ ëª¨ë“ˆ ë¡œë“œ (ê±´ê°• ë¶„ì„, ìš´ë™ ì¶”ì²œ, ë²¡í„° ì €ì¥ì†Œ)
        self._load_service_modules()

    def _load_service_modules(self):
        """ì„œë¹„ìŠ¤ ëª¨ë“ˆ ë™ì  ë¡œë“œ"""
        try:
            from app.core.health_interpreter import (
                interpret_health_data,
                build_health_context_for_llm,
            )

            self.interpret_health_data = interpret_health_data
            self.build_health_context_for_llm = build_health_context_for_llm
            print("âœ… health_interpreter ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
        except ImportError as e:
            print(f"âš ï¸ health_interpreter ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.interpret_health_data = None
            self.build_health_context_for_llm = None

        try:
            from app.core.llm_analysis import run_llm_analysis

            self.run_llm_analysis = run_llm_analysis
            print("âœ… llm_analysis ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
        except ImportError as e:
            print(f"âš ï¸ llm_analysis ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.run_llm_analysis = None

        try:
            from app.core.vector_store import save_daily_summary, collection

            self.save_daily_summary = save_daily_summary
            self.chroma_collection = collection
            print("âœ… vector_store ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
        except ImportError as e:
            print(f"âš ï¸ vector_store ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.save_daily_summary = None
            self.chroma_collection = None

    # ============================================
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° Setup / Cleanup
    # ============================================

    def setup_test_data(self):
        """
        í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ê±´ê°• ë°ì´í„°ë¥¼ ChromaDBì— ì €ì¥
        ì±—ë´‡ RAG í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ í•„ìš”
        """
        if self.save_daily_summary is None:
            print("âš ï¸ vector_store ëª¨ë“ˆì´ ì—†ì–´ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¤ì • ë¶ˆê°€")
            return False

        print("\nğŸ“¦ í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ì„¤ì • ì¤‘...")

        today = datetime.now()
        self.test_data_ids = []

        for sample in SAMPLE_HEALTH_DATA:
            # ë‚ ì§œ ê³„ì‚°
            target_date = today + timedelta(days=sample["date_offset"])
            date_str = target_date.strftime("%Y-%m-%d")

            # summary í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            summary = {
                "created_at": f"{date_str}T12:00:00",
                "platform": "test_evaluation",
                "raw": sample["raw"],
            }

            try:
                result = self.save_daily_summary(
                    summary=summary, user_id=self.user_id, source="test_eval"
                )
                doc_id = result.get("document_id")
                if doc_id:
                    self.test_data_ids.append(doc_id)
                print(f"   âœ… {date_str} ë°ì´í„° ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                print(f"   âŒ {date_str} ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")

        print(f"ğŸ“¦ ì´ {len(self.test_data_ids)}ê°œ ìƒ˜í”Œ ë°ì´í„° ì €ì¥ ì™„ë£Œ\n")
        return True

    def cleanup_test_data(self):
        """
        í…ŒìŠ¤íŠ¸ í›„ ìƒ˜í”Œ ë°ì´í„° ì‚­ì œ
        """
        if self.chroma_collection is None:
            print("âš ï¸ ChromaDB collectionì´ ì—†ì–´ì„œ ì •ë¦¬ ë¶ˆê°€")
            return False

        if not self.test_data_ids:
            print("â„¹ï¸ ì‚­ì œí•  í…ŒìŠ¤íŠ¸ ë°ì´í„° ì—†ìŒ")
            return True

        print("\nğŸ§¹ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬ ì¤‘...")

        try:
            self.chroma_collection.delete(ids=self.test_data_ids)
            print(f"ğŸ§¹ {len(self.test_data_ids)}ê°œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚­ì œ ì™„ë£Œ\n")
            self.test_data_ids = []
            return True
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨: {e}")
            return False

    def run_all(
        self, datasets_dir: str = "evaluation/datasets", cleanup: bool = False
    ) -> dict:
        """
        ëª¨ë“  ì„œë¹„ìŠ¤ í‰ê°€ ì‹¤í–‰

        Args:
            datasets_dir: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ê²½ë¡œ
            cleanup: í…ŒìŠ¤íŠ¸ í›„ ìƒ˜í”Œ ë°ì´í„° ì‚­ì œ ì—¬ë¶€ (ê¸°ë³¸ False - í›„ì† í…ŒìŠ¤íŠ¸ ìœ„í•´ ìœ ì§€)
        """
        print("=" * 60)
        print("ğŸš€ Baseline í‰ê°€ ì‹œì‘")
        print("=" * 60)

        # 0. í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ì„¤ì • (ì±—ë´‡ RAGìš©)
        self.setup_test_data()

        datasets_path = Path(datasets_dir)

        # 1. ê±´ê°• ë¶„ì„ í‰ê°€ (ìƒì²´ ë°ì´í„° ì…ë ¥)
        health_path = datasets_path / "health_data.json"
        if health_path.exists():
            print("\nğŸ“Š ê±´ê°• ë¶„ì„ í‰ê°€ ì¤‘...")
            self.results["health"] = self._run_health_evaluation(health_path)
            print(f"   ì™„ë£Œ: {len(self.results['health'])}ê±´")
        else:
            print(f"\nâš ï¸ ê±´ê°• ë¶„ì„ ë°ì´í„° ì—†ìŒ: {health_path}")

        # 2. ìš´ë™ ì¶”ì²œ í‰ê°€ (ìƒì²´ ë°ì´í„° + ì˜µì…˜ ì…ë ¥)
        exercise_path = datasets_path / "exercise_data.json"
        if exercise_path.exists():
            print("\nğŸƒ ìš´ë™ ì¶”ì²œ í‰ê°€ ì¤‘...")
            self.results["exercise"] = self._run_exercise_evaluation(exercise_path)
            print(f"   ì™„ë£Œ: {len(self.results['exercise'])}ê±´")
        else:
            print(f"\nâš ï¸ ìš´ë™ ì¶”ì²œ ë°ì´í„° ì—†ìŒ: {exercise_path}")

        # 3. ì±—ë´‡ í‰ê°€ (ì§ˆë¬¸ í…ìŠ¤íŠ¸ ì…ë ¥)
        chat_path = datasets_path / "chat_queries.json"
        if chat_path.exists():
            print("\nğŸ’¬ ì±—ë´‡ í‰ê°€ ì¤‘...")
            self.results["chat"] = self._run_chat_evaluation(chat_path)
            print(f"   ì™„ë£Œ: {len(self.results['chat'])}ê±´")
        else:
            print(f"\nâš ï¸ ì±—ë´‡ ë°ì´í„° ì—†ìŒ: {chat_path}")

        # ìš”ì•½ ìƒì„±
        self.summary = self._generate_summary()

        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬ (ì„ íƒ)
        if cleanup:
            self.cleanup_test_data()

        return {"results": self.results, "summary": self.summary}

    # ============================================
    # ê±´ê°• ë¶„ì„ í‰ê°€ (ìƒì²´ ë°ì´í„° â†’ interpret_health_data)
    # ============================================

    def _run_health_evaluation(self, dataset_path: Path) -> list:
        """
        ê±´ê°• ë¶„ì„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        ì…ë ¥: ìƒì²´ ë°ì´í„° (9ê°œ ì§€í‘œ)
        í˜¸ì¶œ: interpret_health_data() ì§ì ‘ í˜¸ì¶œ
        """
        with open(dataset_path, "r", encoding="utf-8") as f:
            dataset = json.load(f)

        results = []

        for test_case in dataset.get("test_cases", []):
            result = self._evaluate_health_analysis(test_case)
            results.append(result)

        return results

    def _evaluate_health_analysis(self, test_case: dict) -> dict:
        """ë‹¨ì¼ ê±´ê°• ë¶„ì„ í‰ê°€"""
        result = {
            "id": test_case["id"],
            "difficulty": test_case.get("difficulty", "medium"),
            "input_data": test_case["input_data"],
            "expected": test_case["expected"],
            "responses": [],
            "times": [],
            "scores": {},
        }

        input_data = test_case["input_data"]
        expected = test_case["expected"]

        # ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰ (ì¼ê´€ì„± ì¸¡ì •)
        for _ in range(EVALUATION_ROUNDS):
            response, elapsed = self._call_health_interpreter(input_data)
            result["responses"].append(response)
            result["times"].append(elapsed)

        # ì‘ë‹µì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°)
        first_response = result["responses"][0]
        if isinstance(first_response, dict):
            response_text = json.dumps(first_response, ensure_ascii=False)
        else:
            response_text = str(first_response)

        # ì ìˆ˜ ê³„ì‚°
        expected_keywords = expected.get("keywords", [])

        result["scores"] = {
            "accuracy": self._calculate_health_accuracy(first_response, expected),
            "keyword_match": ResponseQualityMetrics.keyword_match_score(
                response_text, expected_keywords
            ),
            "consistency": self._calculate_dict_consistency(result["responses"]),
            "condition_match": self._check_condition_match(first_response, expected),
            "avg_time": PerformanceMetrics.calculate_stats(result["times"])["avg"],
            "avg_tokens": PerformanceMetrics.estimate_tokens(response_text),
        }

        return result

    def _call_health_interpreter(self, input_data: dict) -> tuple:
        """ê±´ê°• ë¶„ì„ ëª¨ë“ˆ ì§ì ‘ í˜¸ì¶œ"""
        start = datetime.now()

        if self.interpret_health_data is None:
            elapsed = (datetime.now() - start).total_seconds()
            return {"error": "health_interpreter ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨"}, elapsed

        try:
            result = self.interpret_health_data(input_data)
            elapsed = (datetime.now() - start).total_seconds()
            return result, elapsed
        except Exception as e:
            elapsed = (datetime.now() - start).total_seconds()
            return {"error": str(e)}, elapsed

    def _calculate_health_accuracy(self, response: dict, expected: dict) -> float:
        """ê±´ê°• ë¶„ì„ ì •í™•ë„ ê³„ì‚°"""
        if isinstance(response, dict) and "error" in response:
            return 0.0

        score = 0
        total = 0

        # ì»¨ë””ì…˜ ë ˆë²¨ ë§¤ì¹­ (40ì )
        total += 40
        expected_level = expected.get("condition_level", "")
        if isinstance(response, dict):
            # health_score ê¸°ë°˜ ì»¨ë””ì…˜ íŒì •
            health_score = response.get("health_score", {})
            actual_score = (
                health_score.get("score", 50) if isinstance(health_score, dict) else 50
            )

            # ì ìˆ˜ â†’ ì»¨ë””ì…˜ ë ˆë²¨ ë§¤í•‘
            if actual_score >= 80:
                actual_level = "optimal"
            elif actual_score >= 60:
                actual_level = "good"
            else:
                actual_level = "warning"

            if actual_level == expected_level:
                score += 40
            elif actual_level in ["optimal", "good"] and expected_level in [
                "optimal",
                "good",
            ]:
                score += 20  # ë¶€ë¶„ ì ìˆ˜

        # ìš´ë™ ê°•ë„ ê¶Œì¥ ë§¤ì¹­ (30ì )
        total += 30
        expected_exercise = expected.get("exercise_recommendation", "")
        if isinstance(response, dict):
            exercise_rec = response.get("exercise_recommendation", {})
            if isinstance(exercise_rec, dict):
                rec_level = exercise_rec.get("recommended_level", "")
                if "ê³ ê°•ë„" in expected_exercise and rec_level in ["ê³ ", "ìƒ"]:
                    score += 30
                elif "ì¤‘ê°•ë„" in expected_exercise and rec_level in ["ì¤‘", "ì¤‘ìƒ"]:
                    score += 30
                elif "ì €ê°•ë„" in expected_exercise and rec_level in ["í•˜", "ì €"]:
                    score += 30
                elif "íœ´ì‹" in expected_exercise and rec_level in ["íœ´ì‹", "í•˜"]:
                    score += 30

        # í‚¤ì›Œë“œ í¬í•¨ (30ì )
        total += 30
        expected_keywords = expected.get("keywords", [])
        if expected_keywords:
            response_text = (
                json.dumps(response, ensure_ascii=False)
                if isinstance(response, dict)
                else str(response)
            )
            matched = sum(1 for kw in expected_keywords if kw in response_text)
            score += (matched / len(expected_keywords)) * 30

        return round((score / total) * 100, 1) if total > 0 else 0.0

    def _check_condition_match(self, response: dict, expected: dict) -> bool:
        """ì»¨ë””ì…˜ ë ˆë²¨ ì¼ì¹˜ ì—¬ë¶€"""
        if not isinstance(response, dict) or "error" in response:
            return False

        expected_level = expected.get("condition_level", "")
        health_score = response.get("health_score", {})
        actual_score = (
            health_score.get("score", 50) if isinstance(health_score, dict) else 50
        )

        if actual_score >= 80:
            actual_level = "optimal"
        elif actual_score >= 60:
            actual_level = "good"
        else:
            actual_level = "warning"

        return actual_level == expected_level

    def _calculate_dict_consistency(self, responses: list) -> float:
        """ë”•ì…”ë„ˆë¦¬ ì‘ë‹µë“¤ì˜ ì¼ê´€ì„± ê³„ì‚°"""
        if len(responses) < 2:
            return 1.0

        # ì²« ë²ˆì§¸ ì‘ë‹µê³¼ ë‚˜ë¨¸ì§€ ë¹„êµ
        first = responses[0]
        if not isinstance(first, dict):
            return ResponseQualityMetrics.consistency_score([str(r) for r in responses])

        consistent_count = 0
        for resp in responses[1:]:
            if isinstance(resp, dict):
                # ì£¼ìš” í‚¤ì˜ ê°’ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                first_score = (
                    first.get("health_score", {}).get("score", 0)
                    if isinstance(first.get("health_score"), dict)
                    else 0
                )
                resp_score = (
                    resp.get("health_score", {}).get("score", 0)
                    if isinstance(resp.get("health_score"), dict)
                    else 0
                )

                # ì ìˆ˜ ì°¨ì´ê°€ 5 ì´ë‚´ë©´ ì¼ê´€ì„± ìˆìŒ
                if abs(first_score - resp_score) <= 5:
                    consistent_count += 1

        return consistent_count / (len(responses) - 1)

    # ============================================
    # ìš´ë™ ì¶”ì²œ í‰ê°€ (ìƒì²´ ë°ì´í„° + ì˜µì…˜ â†’ run_llm_analysis)
    # ============================================

    def _run_exercise_evaluation(self, dataset_path: Path) -> list:
        """
        ìš´ë™ ì¶”ì²œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        ì…ë ¥: ìƒì²´ ë°ì´í„° + ë‚œì´ë„/ì‹œê°„ ì˜µì…˜
        í˜¸ì¶œ: run_llm_analysis() ì§ì ‘ í˜¸ì¶œ
        """
        with open(dataset_path, "r", encoding="utf-8") as f:
            dataset = json.load(f)

        results = []

        for test_case in dataset.get("test_cases", []):
            result = self._evaluate_exercise_recommendation(test_case)
            results.append(result)

        return results

    def _evaluate_exercise_recommendation(self, test_case: dict) -> dict:
        """ë‹¨ì¼ ìš´ë™ ì¶”ì²œ í‰ê°€"""
        result = {
            "id": test_case["id"],
            "difficulty": test_case.get("difficulty", "medium"),
            "input_data": test_case["input_data"],
            "options": test_case.get("options", {}),
            "expected": test_case["expected"],
            "responses": [],
            "times": [],
            "scores": {},
        }

        input_data = test_case["input_data"]
        options = test_case.get("options", {"difficulty": "ì¤‘", "duration_min": 30})
        expected = test_case["expected"]

        # ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰ (ì¼ê´€ì„± ì¸¡ì •)
        for _ in range(EVALUATION_ROUNDS):
            response, elapsed = self._call_llm_analysis(input_data, options)
            result["responses"].append(response)
            result["times"].append(elapsed)

        # ì‘ë‹µì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        first_response = result["responses"][0]
        if isinstance(first_response, dict):
            response_text = json.dumps(first_response, ensure_ascii=False)
        else:
            response_text = str(first_response)

        # Fallback ìƒì„¸ ì •ë³´ ì¶”ì¶œ
        fallback_info = self._get_fallback_info(first_response)

        # ì ìˆ˜ ê³„ì‚°
        expected_keywords = expected.get("keywords", [])

        result["scores"] = {
            "accuracy": self._calculate_exercise_accuracy(first_response, expected),
            "keyword_match": ResponseQualityMetrics.keyword_match_score(
                response_text, expected_keywords
            ),
            "consistency": self._calculate_dict_consistency(result["responses"]),
            "has_warmup": self._check_has_warmup(first_response),
            "has_cooldown": self._check_has_cooldown(first_response),
            "intensity_match": self._check_intensity_match(first_response, expected),
            "used_fallback": fallback_info["used_fallback"],
            "fallback_reason": fallback_info["reason"],
            "avg_time": PerformanceMetrics.calculate_stats(result["times"])["avg"],
            "avg_tokens": PerformanceMetrics.estimate_tokens(response_text),
        }

        return result

    def _call_llm_analysis(self, input_data: dict, options: dict) -> tuple:
        """ìš´ë™ ì¶”ì²œ ëª¨ë“ˆ ì§ì ‘ í˜¸ì¶œ"""
        start = datetime.now()

        if self.run_llm_analysis is None:
            elapsed = (datetime.now() - start).total_seconds()
            return {"error": "llm_analysis ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨"}, elapsed

        try:
            # run_llm_analysis í˜¸ì¶œ
            result = self.run_llm_analysis(
                user_id=self.user_id,
                summary={"raw": input_data},
                difficulty_level=options.get("difficulty", "ì¤‘"),
                duration_min=options.get("duration_min", 30),
            )
            elapsed = (datetime.now() - start).total_seconds()
            return result, elapsed
        except Exception as e:
            elapsed = (datetime.now() - start).total_seconds()
            return {"error": str(e)}, elapsed

    def _calculate_exercise_accuracy(self, response: dict, expected: dict) -> float:
        """ìš´ë™ ì¶”ì²œ ì •í™•ë„ ê³„ì‚°"""
        if isinstance(response, dict) and "error" in response:
            return 0.0

        score = 0
        total = 0

        # ë£¨í‹´ ì¡´ì¬ ì—¬ë¶€ (20ì )
        total += 20
        if isinstance(response, dict):
            routine = response.get("ai_recommended_routine", {})
            if routine and routine.get("items"):
                score += 20

        # ì›Œë°ì—… í¬í•¨ (20ì )
        total += 20
        if expected.get("has_warmup", True):
            if self._check_has_warmup(response):
                score += 20

        # ì¿¨ë‹¤ìš´ í¬í•¨ (20ì )
        total += 20
        if expected.get("has_cooldown", True):
            if self._check_has_cooldown(response):
                score += 20

        # ê°•ë„ ë§¤ì¹­ (20ì )
        total += 20
        if self._check_intensity_match(response, expected):
            score += 20

        # í‚¤ì›Œë“œ í¬í•¨ (20ì )
        total += 20
        expected_keywords = expected.get("keywords", [])
        if expected_keywords:
            response_text = (
                json.dumps(response, ensure_ascii=False)
                if isinstance(response, dict)
                else str(response)
            )
            matched = sum(1 for kw in expected_keywords if kw in response_text)
            score += (matched / len(expected_keywords)) * 20

        return round((score / total) * 100, 1) if total > 0 else 0.0

    def _check_has_warmup(self, response: dict) -> bool:
        """ì›Œë°ì—… í¬í•¨ ì—¬ë¶€"""
        if not isinstance(response, dict):
            return False

        response_text = json.dumps(response, ensure_ascii=False).lower()
        warmup_keywords = ["ì›Œë°ì—…", "warm", "ì¤€ë¹„", "ìŠ¤íŠ¸ë ˆì¹­"]
        return any(kw in response_text for kw in warmup_keywords)

    def _check_has_cooldown(self, response: dict) -> bool:
        """ì¿¨ë‹¤ìš´ í¬í•¨ ì—¬ë¶€"""
        if not isinstance(response, dict):
            return False

        response_text = json.dumps(response, ensure_ascii=False).lower()
        cooldown_keywords = ["ì¿¨ë‹¤ìš´", "cool", "ë§ˆë¬´ë¦¬", "ì •ë¦¬"]
        return any(kw in response_text for kw in cooldown_keywords)

    def _check_intensity_match(self, response: dict, expected: dict) -> bool:
        """ìš´ë™ ê°•ë„ ë§¤ì¹­ ì—¬ë¶€"""
        if not isinstance(response, dict):
            return False

        expected_intensity = expected.get("intensity_level", "")
        response_text = json.dumps(response, ensure_ascii=False)

        if "ì €ê°•ë„" in expected_intensity:
            return (
                "í•˜" in response_text
                or "ì €" in response_text
                or "ê°€ë²¼ìš´" in response_text
            )
        elif "ì¤‘ê°•ë„" in expected_intensity or "ì¤‘-ê³ ê°•ë„" in expected_intensity:
            return "ì¤‘" in response_text
        elif "ê³ ê°•ë„" in expected_intensity:
            return "ìƒ" in response_text or "ê³ " in response_text

        return True  # ê¸°ë³¸ê°’

    def _get_fallback_info(self, response: dict) -> dict:
        """
        Fallback ìƒì„¸ ì •ë³´ ì¶”ì¶œ

        Returns:
            {
                "used_fallback": bool,
                "reason": str  # "none", "low_score", "validation_failed", "parse_failed", "data_insufficient", "error"
            }
        """
        if not isinstance(response, dict):
            return {"used_fallback": False, "reason": "none"}

        # health_contextì—ì„œ fallback_reason í™•ì¸
        health_context = response.get("health_context", {})
        fallback_reason = health_context.get("fallback_reason", "")

        if fallback_reason:
            # ì‚¬ìœ  ë¶„ë¥˜
            if (
                "ì ìˆ˜" in fallback_reason
                or "40ì " in fallback_reason
                or "ë¯¸ë§Œ" in fallback_reason
            ):
                return {"used_fallback": True, "reason": "low_score"}
            elif (
                "ê²€ì¦ ì‹¤íŒ¨" in fallback_reason
                or "validation" in fallback_reason.lower()
            ):
                return {"used_fallback": True, "reason": "validation_failed"}
            elif (
                "íŒŒì‹±" in fallback_reason
                or "JSON" in fallback_reason
                or "parse" in fallback_reason.lower()
            ):
                return {"used_fallback": True, "reason": "parse_failed"}
            elif "ë°ì´í„°" in fallback_reason or "ë¶€ì¡±" in fallback_reason:
                return {"used_fallback": True, "reason": "data_insufficient"}
            elif "ì˜¤ë¥˜" in fallback_reason or "error" in fallback_reason.lower():
                return {"used_fallback": True, "reason": "error"}
            else:
                return {"used_fallback": True, "reason": "other"}

        # used_data_rankedì—ì„œ í™•ì¸
        used_data = response.get("used_data_ranked", {})
        primary = used_data.get("primary", "")
        if "fallback" in primary.lower() or "rule" in primary.lower():
            return {"used_fallback": True, "reason": "unknown"}

        # debug_info í™•ì¸ (Fallbackì—ë§Œ ì¡´ì¬)
        if response.get("debug_info"):
            return {"used_fallback": True, "reason": "unknown"}

        # LLM ì„±ê³µ
        return {"used_fallback": False, "reason": "none"}

    def _check_used_fallback(self, response: dict) -> bool:
        """Fallback ì‚¬ìš© ì—¬ë¶€ (ê°„ë‹¨ ë²„ì „)"""
        return self._get_fallback_info(response)["used_fallback"]

    # ============================================
    # ì±—ë´‡ í‰ê°€ (ì§ˆë¬¸ í…ìŠ¤íŠ¸ â†’ /api/chat API)
    # ============================================

    def _run_chat_evaluation(self, dataset_path: Path) -> list:
        """
        ì±—ë´‡ ëŒ€í™” í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        ì…ë ¥: ì§ˆë¬¸ í…ìŠ¤íŠ¸ + ìºë¦­í„°
        í˜¸ì¶œ: /api/chat API
        """
        with open(dataset_path, "r", encoding="utf-8") as f:
            dataset = json.load(f)

        results = []

        for test_case in dataset.get("test_cases", []):
            result = self._evaluate_chat(test_case)
            results.append(result)

        return results

    def _evaluate_chat(self, test_case: dict) -> dict:
        """ë‹¨ì¼ ì±—ë´‡ í‰ê°€"""
        result = {
            "id": test_case["id"],
            "difficulty": test_case.get("difficulty", "medium"),
            "input_data": test_case["input_data"],
            "expected": test_case["expected"],
            "responses": [],
            "times": [],
            "scores": {},
        }

        input_data = test_case["input_data"]
        message = input_data.get("message", "")
        character = input_data.get("character", "devil_coach")
        expected = test_case["expected"]

        # ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰ (ì¼ê´€ì„± ì¸¡ì •)
        for _ in range(EVALUATION_ROUNDS):
            response, elapsed = self._call_chat_api(message, character)
            result["responses"].append(response)
            result["times"].append(elapsed)

        # ì ìˆ˜ ê³„ì‚°
        expected_keywords = expected.get("keywords", [])
        first_response = result["responses"][0]

        result["scores"] = {
            "accuracy": self._calculate_chat_accuracy(
                first_response, expected, character
            ),
            "keyword_match": ResponseQualityMetrics.keyword_match_score(
                first_response, expected_keywords
            ),
            "consistency": ResponseQualityMetrics.consistency_score(
                result["responses"]
            ),
            "length_score": ResponseQualityMetrics.response_length_score(
                first_response
            ),
            "tone_match": self._check_tone_match(first_response, expected, character),
            "rag_utilization": self._check_rag_utilization(first_response),
            "avg_time": PerformanceMetrics.calculate_stats(result["times"])["avg"],
            "avg_tokens": PerformanceMetrics.estimate_tokens(first_response),
        }

        return result

    def _check_rag_utilization(self, response: str) -> float:
        """
        RAG í™œìš©ë„ ì¸¡ì •
        ì‘ë‹µì— ê±´ê°• ë°ì´í„° ê´€ë ¨ êµ¬ì²´ì  ìˆ˜ì¹˜ë‚˜ ë‚ ì§œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        """
        if response.startswith("Error:"):
            return 0.0

        score = 0.0

        # 1. ìˆ˜ì¹˜ ì–¸ê¸‰ (30ì )
        import re

        numbers = re.findall(r"\d+\.?\d*", response)
        if len(numbers) >= 2:
            score += 0.3
        elif len(numbers) >= 1:
            score += 0.15

        # 2. ê±´ê°• ê´€ë ¨ í‚¤ì›Œë“œ (30ì )
        health_keywords = [
            "ìˆ˜ë©´",
            "ê±¸ìŒ",
            "ì‹¬ë°•",
            "ì¹¼ë¡œë¦¬",
            "ìš´ë™",
            "ì»¨ë””ì…˜",
            "ì²´ì¤‘",
            "ì‚°ì†Œ",
        ]
        matched = sum(1 for kw in health_keywords if kw in response)
        score += min(0.3, matched * 0.1)

        # 3. ì‹œê°„/ë‚ ì§œ ì–¸ê¸‰ (20ì )
        time_keywords = ["ì˜¤ëŠ˜", "ì–´ì œ", "ìµœê·¼", "ì§€ë‚œ", "ì´ë²ˆ ì£¼", "ì¼ì£¼ì¼"]
        if any(kw in response for kw in time_keywords):
            score += 0.2

        # 4. ê°œì¸í™”ëœ ì¡°ì–¸ (20ì )
        personalized_keywords = ["ë‹¹ì‹ ", "íšŒì›ë‹˜", "ë°ì´í„°", "ê¸°ë¡", "ë¶„ì„"]
        if any(kw in response for kw in personalized_keywords):
            score += 0.2

        return round(score, 2)

    def _call_chat_api(self, message: str, character: str = "devil_coach") -> tuple:
        """ì±—ë´‡ API í˜¸ì¶œ"""
        url = f"{self.base_url}/api/chat"
        payload = {
            "user_id": self.user_id,
            "message": message,
            "character": character,
        }

        start = datetime.now()
        try:
            response = requests.post(url, json=payload, timeout=30)
            response.raise_for_status()
            result = response.json()
            elapsed = (datetime.now() - start).total_seconds()
            return result.get("response", ""), elapsed
        except requests.exceptions.ConnectionError:
            elapsed = (datetime.now() - start).total_seconds()
            return (
                "Error: ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.",
                elapsed,
            )
        except Exception as e:
            elapsed = (datetime.now() - start).total_seconds()
            return f"Error: {str(e)}", elapsed

    def _calculate_chat_accuracy(
        self, response: str, expected: dict, character: str
    ) -> float:
        """ì±—ë´‡ ì‘ë‹µ ì •í™•ë„ ê³„ì‚°"""
        if response.startswith("Error:"):
            return 0.0

        score = 0
        total = 0

        # í‚¤ì›Œë“œ ë§¤ì¹­ (50ì )
        total += 50
        expected_keywords = expected.get("keywords", [])
        if expected_keywords:
            matched = sum(1 for kw in expected_keywords if kw in response)
            score += (matched / len(expected_keywords)) * 50

        # í†¤ ë§¤ì¹­ (30ì )
        total += 30
        if self._check_tone_match(response, expected, character):
            score += 30

        # ì‘ë‹µ ê¸¸ì´ ì ì ˆì„± (20ì )
        total += 20
        length_score = ResponseQualityMetrics.response_length_score(response)
        score += length_score * 20

        return round((score / total) * 100, 1) if total > 0 else 0.0

    def _check_tone_match(self, response: str, expected: dict, character: str) -> bool:
        """í˜ë¥´ì†Œë‚˜ í†¤ ë§¤ì¹­ ì—¬ë¶€"""
        expected_tone = expected.get("tone", "")

        # ìºë¦­í„°ë³„ í†¤ í‚¤ì›Œë“œ
        tone_keywords = {
            "devil_coach": {
                "tough_love": ["í•´ì•¼ì§€", "ë³€ëª…", "í•‘ê³„", "ë‹¹ì¥", "ë­í•´", "ê²Œìœ¼ë¦„"]
            },
            "angel_coach": {
                "supportive": ["ì˜í–ˆì–´", "ëŒ€ë‹¨í•´", "ë©‹ì ¸", "ìµœê³ ", "í›Œë¥­", "ì‘ì›"]
            },
            "booster_coach": {
                "encouraging": ["í•  ìˆ˜ ìˆì–´", "íŒŒì´íŒ…", "ë¯¿ì–´", "ê´œì°®ì•„", "í˜ë‚´"]
            },
        }

        character_tones = tone_keywords.get(character, {})
        keywords = character_tones.get(expected_tone, [])

        if not keywords:
            return True  # í‚¤ì›Œë“œ ì—†ìœ¼ë©´ í†µê³¼

        return any(kw in response for kw in keywords)

    # ============================================
    # ìš”ì•½ ë° ì €ì¥
    # ============================================

    def _generate_summary(self) -> dict:
        """ìš”ì•½ í†µê³„ ìƒì„±"""
        summary = {
            "timestamp": datetime.now().isoformat(),
            "stage": "baseline",
            "total_queries": 0,
            "by_service": {},
        }

        for service, results in self.results.items():
            if not results:
                continue

            service_summary = {
                "count": len(results),
                "avg_accuracy": 0,
                "avg_keyword_match": 0,
                "avg_consistency": 0,
                "avg_time": 0,
                "avg_tokens": 0,
                "by_difficulty": {},
            }

            if results:
                service_summary["avg_accuracy"] = round(
                    sum(r["scores"]["accuracy"] for r in results) / len(results), 2
                )
                service_summary["avg_keyword_match"] = round(
                    sum(r["scores"]["keyword_match"] for r in results) / len(results), 4
                )
                service_summary["avg_consistency"] = round(
                    sum(r["scores"]["consistency"] for r in results) / len(results), 4
                )
                service_summary["avg_time"] = round(
                    sum(r["scores"]["avg_time"] for r in results) / len(results), 4
                )
                service_summary["avg_tokens"] = round(
                    sum(r["scores"]["avg_tokens"] for r in results) / len(results), 0
                )

                # RAG í™œìš©ë„ (ì±—ë´‡ë§Œ)
                if service == "chat":
                    rag_scores = [
                        r["scores"].get("rag_utilization", 0) for r in results
                    ]
                    if rag_scores:
                        service_summary["avg_rag_utilization"] = round(
                            sum(rag_scores) / len(rag_scores), 4
                        )

                # Fallback ë¹„ìœ¨ (ìš´ë™ ì¶”ì²œë§Œ)
                if service == "exercise":
                    # Fallback ì‚¬ìœ ë³„ ì§‘ê³„
                    fallback_by_reason = {
                        "low_score": 0,
                        "validation_failed": 0,
                        "parse_failed": 0,
                        "data_insufficient": 0,
                        "error": 0,
                        "other": 0,
                    }

                    llm_results = []
                    fallback_results = []

                    for r in results:
                        if r["scores"].get("used_fallback", False):
                            reason = r["scores"].get("fallback_reason", "other")
                            if reason in fallback_by_reason:
                                fallback_by_reason[reason] += 1
                            else:
                                fallback_by_reason["other"] += 1
                            fallback_results.append(r)
                        else:
                            llm_results.append(r)

                    fallback_count = len(fallback_results)
                    llm_count = len(llm_results)

                    service_summary["fallback_rate"] = (
                        round(fallback_count / len(results), 4) if results else 0
                    )
                    service_summary["fallback_count"] = fallback_count
                    service_summary["llm_count"] = llm_count
                    service_summary["fallback_by_reason"] = {
                        k: v for k, v in fallback_by_reason.items() if v > 0
                    }

                    # LLMë§Œ ì‚¬ìš©í•œ ì¼€ì´ìŠ¤ ì •í™•ë„
                    if llm_results:
                        service_summary["llm_accuracy"] = round(
                            sum(r["scores"]["accuracy"] for r in llm_results)
                            / len(llm_results),
                            2,
                        )
                    else:
                        service_summary["llm_accuracy"] = None

                    # Fallback ì¼€ì´ìŠ¤ ì •í™•ë„
                    if fallback_results:
                        service_summary["fallback_accuracy"] = round(
                            sum(r["scores"]["accuracy"] for r in fallback_results)
                            / len(fallback_results),
                            2,
                        )
                    else:
                        service_summary["fallback_accuracy"] = None

                # ë‚œì´ë„ë³„ í†µê³„
                for difficulty in ["easy", "medium", "hard"]:
                    diff_results = [r for r in results if r["difficulty"] == difficulty]
                    if diff_results:
                        service_summary["by_difficulty"][difficulty] = {
                            "count": len(diff_results),
                            "avg_accuracy": round(
                                sum(r["scores"]["accuracy"] for r in diff_results)
                                / len(diff_results),
                                2,
                            ),
                        }

            summary["by_service"][service] = service_summary
            summary["total_queries"] += service_summary["count"]

        # ì „ì²´ í‰ê· 
        all_results = (
            self.results["health"] + self.results["exercise"] + self.results["chat"]
        )
        if all_results:
            summary["overall"] = {
                "avg_accuracy": round(
                    sum(r["scores"]["accuracy"] for r in all_results)
                    / len(all_results),
                    2,
                ),
                "avg_time": round(
                    sum(r["scores"]["avg_time"] for r in all_results)
                    / len(all_results),
                    4,
                ),
                "avg_tokens": round(
                    sum(r["scores"]["avg_tokens"] for r in all_results)
                    / len(all_results),
                    0,
                ),
            }

        return summary

    def save_results(self, output_dir: str = None) -> Path:
        """ê²°ê³¼ ì €ì¥"""
        if output_dir is None:
            output_dir = f"{RESULTS_DIR}/baseline"

        Path(output_dir).mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = Path(output_dir) / f"results_{timestamp}.json"

        output_data = {
            "metadata": {
                "stage": "baseline",
                "timestamp": datetime.now().isoformat(),
                "api_base_url": self.base_url,
                "evaluation_rounds": EVALUATION_ROUNDS,
            },
            "summary": self.summary,
            "results": self.results,
        }

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
        return output_path

    def print_summary(self):
        """ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 60)
        print("ğŸ“Š Baseline í‰ê°€ ìš”ì•½")
        print("=" * 60)

        print(f"\nì´ í…ŒìŠ¤íŠ¸: {self.summary.get('total_queries', 0)}ê±´")

        for service, stats in self.summary.get("by_service", {}).items():
            service_name = {
                "health": "ê±´ê°• ë¶„ì„",
                "exercise": "ìš´ë™ ì¶”ì²œ",
                "chat": "ì±—ë´‡",
            }.get(service, service)

            print(f"\n[{service_name}] ({stats['count']}ê±´)")
            print(f"   ì •í™•ë„: {stats['avg_accuracy']:.1f}%")
            print(f"   í‚¤ì›Œë“œ ë§¤ì¹­: {stats['avg_keyword_match']:.2f}")
            print(f"   ì¼ê´€ì„±: {stats['avg_consistency']:.2f}")
            print(f"   ì‘ë‹µ ì‹œê°„: {stats['avg_time']:.2f}ì´ˆ")
            print(f"   í‰ê·  í† í°: {stats['avg_tokens']:.0f}")

            # ë‚œì´ë„ë³„ ì •í™•ë„
            if stats.get("by_difficulty"):
                print(f"   ë‚œì´ë„ë³„ ì •í™•ë„:")
                for diff, diff_stats in stats["by_difficulty"].items():
                    diff_name = {
                        "easy": "ì‰¬ì›€",
                        "medium": "ë³´í†µ",
                        "hard": "ì–´ë ¤ì›€",
                    }.get(diff, diff)
                    print(
                        f"      - {diff_name}: {diff_stats['avg_accuracy']:.1f}% ({diff_stats['count']}ê±´)"
                    )

            # RAG í™œìš©ë„ (ì±—ë´‡ë§Œ)
            if "avg_rag_utilization" in stats:
                print(f"   RAG í™œìš©ë„: {stats['avg_rag_utilization']:.2f}")

            # Fallback ë¹„ìœ¨ (ìš´ë™ ì¶”ì²œë§Œ)
            if "fallback_rate" in stats:
                print(
                    f"   Fallback ë¹„ìœ¨: {stats['fallback_rate']*100:.1f}% ({stats['fallback_count']}ê±´ Fallback / {stats['llm_count']}ê±´ LLM)"
                )

                # ì‚¬ìœ ë³„ ìƒì„¸
                if stats.get("fallback_by_reason"):
                    print(f"   Fallback ì‚¬ìœ :")
                    reason_names = {
                        "low_score": "ì ìˆ˜ ë‚®ìŒ (ê°œì„  ë¶ˆê°€)",
                        "validation_failed": "ê²€ì¦ ì‹¤íŒ¨ (ê°œì„  ê°€ëŠ¥)",
                        "parse_failed": "íŒŒì‹± ì‹¤íŒ¨ (ê°œì„  ê°€ëŠ¥)",
                        "data_insufficient": "ë°ì´í„° ë¶€ì¡±",
                        "error": "ì˜¤ë¥˜ ë°œìƒ",
                        "other": "ê¸°íƒ€",
                    }
                    for reason, count in stats["fallback_by_reason"].items():
                        reason_name = reason_names.get(reason, reason)
                        print(f"      - {reason_name}: {count}ê±´")

                # LLM vs Fallback ì •í™•ë„ ë¹„êµ
                if stats.get("llm_accuracy") is not None:
                    print(f"   LLM ì •í™•ë„: {stats['llm_accuracy']:.1f}%")
                if stats.get("fallback_accuracy") is not None:
                    print(f"   Fallback ì •í™•ë„: {stats['fallback_accuracy']:.1f}%")

        if "overall" in self.summary:
            print(f"\n[ì „ì²´ í‰ê· ]")
            print(f"   ì •í™•ë„: {self.summary['overall']['avg_accuracy']:.1f}%")
            print(f"   ì‘ë‹µ ì‹œê°„: {self.summary['overall']['avg_time']:.2f}ì´ˆ")
            print(f"   í‰ê·  í† í°: {self.summary['overall']['avg_tokens']:.0f}")


if __name__ == "__main__":
    runner = BaselineRunner()
    runner.run_all()
    runner.print_summary()
    runner.save_results()
