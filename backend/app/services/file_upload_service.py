import os, shutil, tempfile, uuid
from pathlib import Path
from datetime import datetime
from fastapi import UploadFile, HTTPException
import asyncio
from concurrent.futures import ThreadPoolExecutor

from app.core.unzipper import extract_zip_to_temp
from app.core.db_to_json import db_to_json
from app.core.db_parser import parse_db_json_to_raw_data_by_day

from app.utils.preprocess import preprocess_health_json
from app.core.vector_store import save_daily_summaries_batch
from app.core.llm_analysis import run_llm_analysis

# ë¹„ë™ê¸° ì²˜ë¦¬ìš© Executor
executor = ThreadPoolExecutor(max_workers=4)

# ============================================================
# ZIP ì €ì¥ ê²½ë¡œ ì„¤ì •
# ============================================================
BASE_DIR = Path(__file__).resolve().parent.parent.parent  # backend/
ZIP_DATA_DIR = BASE_DIR / "zip_data"
UPLOADS_DIR = ZIP_DATA_DIR / "uploads"
EXTRACTED_DIR = ZIP_DATA_DIR / "extracted"

# ë””ë ‰í† ë¦¬ ìƒì„±
UPLOADS_DIR.mkdir(parents=True, exist_ok=True)
EXTRACTED_DIR.mkdir(parents=True, exist_ok=True)

# ============================================================
# íŒŒì‹± ì„¤ì • (LangChain ë¦¬íŒ©í† ë§)
# ============================================================
PARSE_RECENT_DAYS = 30  # ìµœê·¼ Nì¼ë§Œ íŒŒì‹± (ì„±ëŠ¥ ìµœì í™”)


class FileUploadService:
    """
    ZIP/DB íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬ ì„œë¹„ìŠ¤

    LangChain ë¦¬íŒ©í† ë§ ê°œì„  ì‚¬í•­:
    1. ìµœê·¼ 30ì¼ë§Œ íŒŒì‹± (420ì¼ â†’ 30ì¼, 93% ê°ì†Œ)
    2. ì›ë³¸ ZIPì€ ê·¸ëŒ€ë¡œ ë³´ê´€ (í•„ìš”ì‹œ ì¶”í›„ íŒŒì‹± ê°€ëŠ¥)
    3. íŒŒì‹±/ì„ë² ë”© ì‹œê°„ ëŒ€í­ ë‹¨ì¶•
    """

    @staticmethod
    def get_or_create_user_id(user_id: str | None):
        if not user_id or not user_id.strip():
            return str(uuid.uuid4())
        return user_id

    @staticmethod
    async def run_blocking(func, *args):
        """ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(executor, lambda: func(*args))

    @staticmethod
    def detect_platform(filename: str, db_json: dict) -> str:
        """
        í”Œë«í¼ ìë™ ê°ì§€

        Returns:
            "apple" or "samsung" or "unknown"
        """
        filename_lower = filename.lower()

        # íŒŒì¼ëª…ìœ¼ë¡œ ê°ì§€
        if "healthconnect" in filename_lower or "samsung" in filename_lower:
            return "samsung"
        elif (
            "export" in filename_lower
            or "apple" in filename_lower
            or "health" in filename_lower
        ):
            return "apple"

        # DB êµ¬ì¡°ë¡œ ê°ì§€ (Samsung Health Connect íŠ¹ì§•)
        if db_json:
            samsung_tables = [
                "steps_record_table",
                "distance_record_table",
                "heart_rate_record_table",
            ]
            if all(table in db_json for table in samsung_tables):
                return "samsung"

        return "unknown"

    @staticmethod
    def filter_recent_days(
        raw_by_day: dict, recent_days: int = PARSE_RECENT_DAYS
    ) -> dict:
        """
        ìµœê·¼ Nì¼ì¹˜ ë°ì´í„°ë§Œ í•„í„°ë§

        Args:
            raw_by_day: ì „ì²´ ë‚ ì§œë³„ ë°ì´í„° {date_int: raw_data}
            recent_days: ê°€ì ¸ì˜¬ ìµœê·¼ ì¼ìˆ˜ (ê¸°ë³¸ 30ì¼)

        Returns:
            ìµœê·¼ Nì¼ì¹˜ ë°ì´í„°ë§Œ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬
        """
        if not raw_by_day:
            return {}

        # ë‚ ì§œ ê¸°ì¤€ ì •ë ¬ (ìµœì‹ ìˆœ)
        sorted_dates = sorted(raw_by_day.keys(), reverse=True)

        # ìµœê·¼ Nì¼ë§Œ ì„ íƒ
        recent_dates = sorted_dates[:recent_days]

        # í•„í„°ë§ëœ ë°ì´í„° ë°˜í™˜
        filtered = {date: raw_by_day[date] for date in recent_dates}

        return filtered

    async def process_file(
        self,
        file: UploadFile,
        user_id: str | None,
        difficulty: str,
        duration: int,
    ):
        user_id = self.get_or_create_user_id(user_id)

        # ì‚¬ìš©ìë³„ íƒ€ì„ìŠ¤íƒ¬í”„ ë””ë ‰í† ë¦¬
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        user_short = user_id.replace("@", "_").replace(".", "_")

        temp_dir = str(EXTRACTED_DIR / f"{user_short}_{timestamp}")
        os.makedirs(temp_dir, exist_ok=True)

        temp_path = os.path.join(temp_dir, file.filename)

        try:
            print(f"[INFO] íŒŒì¼ ì—…ë¡œë“œ ì‹œì‘: {file.filename}")

            # 1ï¸âƒ£ íŒŒì¼ ì €ì¥
            with open(temp_path, "wb") as buffer:
                buffer.write(await file.read())

            # ì›ë³¸ íŒŒì¼ ë³´ê´€
            original_save_name = f"{user_short}_{timestamp}_{file.filename}"
            original_save_path = UPLOADS_DIR / original_save_name
            shutil.copy2(temp_path, original_save_path)
            print(f"[INFO] ì›ë³¸ íŒŒì¼ ì €ì¥: {original_save_path}")

            # 2ï¸âƒ£ ZIP ë˜ëŠ” DB íŒë³„
            if file.filename.lower().endswith(".zip"):
                print("[INFO] ZIP íŒŒì¼ ì••ì¶• í•´ì œ ì¤‘...")
                db_path = await self.run_blocking(extract_zip_to_temp, temp_path)
            elif file.filename.lower().endswith(".db"):
                db_path = temp_path
            else:
                raise HTTPException(400, "ZIP ë˜ëŠ” DB íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

            if not db_path:
                raise HTTPException(500, "DB íŒŒì¼ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            # 3ï¸âƒ£ DB â†’ JSON (ë¹„ë™ê¸° ì²˜ë¦¬)
            print("[INFO] DB íŒŒì‹± ì¤‘...")
            raw_db_json = await self.run_blocking(db_to_json, db_path)

            # í”Œë«í¼ ê°ì§€
            platform = self.detect_platform(file.filename, raw_db_json)
            print(f"[INFO] ê°ì§€ëœ í”Œë«í¼: {platform}")

            # 4ï¸âƒ£ ë‚ ì§œë³„ raw ì¶”ì¶œ (ì „ì²´)
            print("[INFO] ë‚ ì§œë³„ ë°ì´í„° ì¶”ì¶œ ì¤‘...")
            raw_by_day_all = await self.run_blocking(
                parse_db_json_to_raw_data_by_day, raw_db_json
            )

            if not raw_by_day_all:
                raise HTTPException(
                    500, "DB Parserê°€ ê±´ê°• ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                )

            total_days_in_file = len(raw_by_day_all)
            all_dates = sorted(raw_by_day_all.keys())

            print(f"[INFO] íŒŒì¼ ë‚´ ì´ {total_days_in_file}ì¼ì¹˜ ë°ì´í„° ë°œê²¬")
            if all_dates:
                print(f"[INFO] ì „ì²´ ë‚ ì§œ ë²”ìœ„: {all_dates[0]} ~ {all_dates[-1]}")

            # ============================================================
            # ğŸš€ LangChain ìµœì í™”: ìµœê·¼ 30ì¼ë§Œ íŒŒì‹±
            # ============================================================
            raw_by_day = self.filter_recent_days(raw_by_day_all, PARSE_RECENT_DAYS)
            total_days = len(raw_by_day)
            dates = sorted(raw_by_day.keys())

            print(f"[INFO] âœ… ìµœê·¼ {PARSE_RECENT_DAYS}ì¼ë§Œ ì²˜ë¦¬: {total_days}ì¼")
            print(f"[INFO] ì²˜ë¦¬ ë‚ ì§œ ë²”ìœ„: {dates[0]} ~ {dates[-1]}")
            print(
                f"[INFO] ìŠ¤í‚µëœ ë°ì´í„°: {total_days_in_file - total_days}ì¼ (ì›ë³¸ ZIPì— ë³´ê´€)"
            )

            # 5ï¸âƒ£ ìµœì‹  ë‚ ì§œ ê²°ì •
            latest_date = max(raw_by_day.keys())
            latest_raw = raw_by_day[latest_date]

            # 6ï¸âƒ£ ìµœì‹  1ì¼ì¹˜ summary (ë¶„ì„ìš©)
            print("[INFO] ìµœì‹  ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
            latest_summary = await self.run_blocking(
                preprocess_health_json, latest_raw, latest_date, platform
            )

            # 7ï¸âƒ£ ìµœê·¼ 30ì¼ summary â†’ Vector DB ë°°ì¹˜ ì €ì¥
            print(f"[INFO] VectorDBì— {total_days}ì¼ì¹˜ ë°ì´í„° ë°°ì¹˜ ì €ì¥ ì¤‘...")

            all_summaries = []
            for date_int, raw in raw_by_day.items():
                daily_summary = await self.run_blocking(
                    preprocess_health_json,
                    raw,
                    date_int,
                    platform,
                )
                all_summaries.append(daily_summary)

            source = f"zip_{platform}"
            await self.run_blocking(
                save_daily_summaries_batch, all_summaries, user_id, source
            )

            print(
                f"[SUCCESS] {total_days}ì¼ì¹˜ ë°ì´í„° VectorDB ì €ì¥ ì™„ë£Œ (í”Œë«í¼: {platform})"
            )

            # 8ï¸âƒ£ LLM ë¶„ì„ (ìµœì‹  ë°ì´í„°ë§Œ)
            print("[INFO] LLM ë¶„ì„ ì‹¤í–‰ ì¤‘...")
            llm_result = await self.run_blocking(
                run_llm_analysis,
                latest_summary,
                user_id,
                difficulty,
                duration,
            )

            # âœ… ê±´ê°• ë¶„ì„ (health_info) ì¶”ê°€
            from app.core.health_interpreter import interpret_health_data

            health_info = await self.run_blocking(
                interpret_health_data, latest_summary.get("raw", {})
            )

            print("[SUCCESS] ë¶„ì„ ì™„ë£Œ")

            # ì €ì¥ ì •ë³´ ë¡œê·¸
            print(f"\n{'='*70}")
            print(f"ğŸ“¦ íŒŒì¼ ì €ì¥ ì •ë³´:")
            print(f"  â€¢ íŒŒì¼ íƒ€ì…: {file.filename.split('.')[-1].upper()}")
            print(f"  â€¢ ì›ë³¸ íŒŒì¼: {original_save_path}")
            print(f"  â€¢ ì••ì¶• í•´ì œ: {temp_dir}")
            print(f"  â€¢ í”Œë«í¼: {platform}")
            print(f"  â€¢ íŒŒì¼ ë‚´ ì „ì²´: {total_days_in_file}ì¼")
            print(f"  â€¢ ì‹¤ì œ ì²˜ë¦¬: {total_days}ì¼ (ìµœê·¼ {PARSE_RECENT_DAYS}ì¼)")
            print(f"  â€¢ ì²˜ë¦¬ ë²”ìœ„: {dates[0]} ~ {dates[-1]}")
            print(f"{'='*70}\n")

            return {
                "message": "ZIP/DB ì—…ë¡œë“œ ë° ë¶„ì„ ì„±ê³µ",
                "user_id": user_id,
                "total_days_in_file": total_days_in_file,
                "total_days_saved": total_days,
                "date_range": f"{dates[0]} ~ {dates[-1]}" if dates else "",
                "latest_date": latest_date,
                "platform": platform,
                "summary": latest_summary,
                "health_info": health_info,
                "llm_result": llm_result,
                "optimization_info": {
                    "parse_limit_days": PARSE_RECENT_DAYS,
                    "skipped_days": total_days_in_file - total_days,
                },
                "file_info": {
                    "file_type": file.filename.split(".")[-1],
                    "original_path": str(original_save_path),
                    "extract_dir": temp_dir,
                },
            }

        except HTTPException:
            raise
        except Exception as e:
            print(f"[ERROR] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            import traceback

            traceback.print_exc()
            raise HTTPException(500, f"ZIP/DB ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

        finally:
            # 9ï¸âƒ£ ì´ì „ ë°ì´í„° ì •ë¦¬ + í˜„ì¬ ë°ì´í„° ë³´ì¡´
            try:
                # í˜„ì¬ ì‚¬ìš©ìì˜ ëª¨ë“  ì¶”ì¶œ ë””ë ‰í† ë¦¬ ì°¾ê¸°
                user_pattern = f"{user_short}_*"
                user_dirs = list(EXTRACTED_DIR.glob(user_pattern))

                # í˜„ì¬ ë””ë ‰í† ë¦¬ ì œì™¸
                current_dir = Path(temp_dir)
                old_dirs = [d for d in user_dirs if d != current_dir]

                # ì´ì „ ì¶”ì¶œ ë””ë ‰í† ë¦¬ ì‚­ì œ
                for old_dir in old_dirs:
                    print(f"[INFO] ì´ì „ ë°ì´í„° ì‚­ì œ: {old_dir.name}")
                    shutil.rmtree(old_dir)

                # ê°™ì€ ìœ ì €ì˜ ì´ì „ ì›ë³¸ íŒŒì¼ ì‚­ì œ
                file_pattern = f"{user_short}_*.*"
                old_files = list(UPLOADS_DIR.glob(file_pattern))

                # í˜„ì¬ íŒŒì¼ ì œì™¸
                current_file = UPLOADS_DIR / original_save_name
                old_files = [f for f in old_files if f != current_file]

                for old_file in old_files:
                    print(f"[INFO] ì´ì „ ì›ë³¸ íŒŒì¼ ì‚­ì œ: {old_file.name}")
                    old_file.unlink()

                print(f"[INFO] ìµœì‹  ë°ì´í„° ë³´ì¡´: {temp_dir}")
                print(f"[INFO] ìµœì‹  ì›ë³¸ ë³´ì¡´: {original_save_path}")

            except Exception as e:
                print(f"[WARN] ì´ì „ ë°ì´í„° ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {str(e)}")
